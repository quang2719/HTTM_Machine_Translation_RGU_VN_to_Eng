{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9777117,"sourceType":"datasetVersion","datasetId":5879313}],"dockerImageVersionId":30302,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Vietnamese to English MT\n","metadata":{}},{"cell_type":"markdown","source":"\n* I. Data Reading and Tokenization\n* II. Data Visualization\n* III. Model Defination\n* IV. Model Storage \n* V. Evaluating Elbow\n* VI. Demo","metadata":{}},{"cell_type":"markdown","source":"### Import library","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, GRU, Embedding\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-02T16:32:31.506594Z","iopub.execute_input":"2024-11-02T16:32:31.507297Z","iopub.status.idle":"2024-11-02T16:32:31.514000Z","shell.execute_reply.started":"2024-11-02T16:32:31.507260Z","shell.execute_reply":"2024-11-02T16:32:31.512919Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# I. Data Reading and Tokenization","metadata":{}},{"cell_type":"markdown","source":"## 1.1 Reading training set","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/my-data/new_train_ds.csv\")\ndata = data.dropna()\ndata.head(10)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2024-11-02T16:32:31.518604Z","iopub.execute_input":"2024-11-02T16:32:31.518945Z","iopub.status.idle":"2024-11-02T16:32:45.833197Z","shell.execute_reply.started":"2024-11-02T16:32:31.518902Z","shell.execute_reply":"2024-11-02T16:32:45.832212Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1.2 Reading test set","metadata":{}},{"cell_type":"code","source":"test_data = pd.read_csv(\"/kaggle/input/my-data/test_ds.csv\")\ntest_data = test_data.dropna()\ndata.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T16:48:43.671823Z","iopub.execute_input":"2024-11-02T16:48:43.672215Z","iopub.status.idle":"2024-11-02T16:48:43.788470Z","shell.execute_reply.started":"2024-11-02T16:48:43.672180Z","shell.execute_reply":"2024-11-02T16:48:43.787532Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(data.shape)\nprint(test_data.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T16:48:43.931394Z","iopub.execute_input":"2024-11-02T16:48:43.931869Z","iopub.status.idle":"2024-11-02T16:48:43.937157Z","shell.execute_reply.started":"2024-11-02T16:48:43.931832Z","shell.execute_reply":"2024-11-02T16:48:43.936151Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1.3 Addining start and end token","metadata":{}},{"cell_type":"code","source":"# Append <START> and <END> to each english sentence\nSTART = 'ssss '\nEND = ' eeee'\n\ndata['en'] = data['en'].apply(lambda x: START+x+END)\ndata.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T16:49:51.769682Z","iopub.execute_input":"2024-11-02T16:49:51.770462Z","iopub.status.idle":"2024-11-02T16:49:53.430102Z","shell.execute_reply.started":"2024-11-02T16:49:51.770424Z","shell.execute_reply":"2024-11-02T16:49:53.429114Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"eng_text = data['en'].tolist()\nvi_text = data['vi'].tolist()\n\nprint(eng_text[3])\nprint(vi_text[3])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T16:51:38.492637Z","iopub.execute_input":"2024-11-02T16:51:38.493045Z","iopub.status.idle":"2024-11-02T16:51:38.705906Z","shell.execute_reply.started":"2024-11-02T16:51:38.493011Z","shell.execute_reply":"2024-11-02T16:51:38.704774Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_words = 10000\nclass TokenizerWrap(Tokenizer):\n    \"\"\"Wrap the Tokenizer-class from Keras with more functionality.\"\"\"\n    \n    def __init__(self, texts, padding,\n                 reverse=False, num_words=None):\n        \"\"\"\n        :param texts: List of strings. This is the data-set.\n        :param padding: Either 'post' or 'pre' padding.\n        :param reverse: Boolean whether to reverse token-lists.\n        :param num_words: Max number of words to use.\n        \"\"\"\n\n        Tokenizer.__init__(self, num_words=num_words)\n\n        # Create the vocabulary from the texts.\n        self.fit_on_texts(texts)\n\n        # Create inverse lookup from integer-tokens to words.\n        self.index_to_word = dict(zip(self.word_index.values(),\n                                      self.word_index.keys()))\n\n        # Convert all texts to lists of integer-tokens.\n        # Note that the sequences may have different lengths.\n        self.tokens = self.texts_to_sequences(texts)\n\n        if reverse:\n            # Reverse the token-sequences.\n            self.tokens = [list(reversed(x)) for x in self.tokens]\n        \n            # Sequences that are too long should now be truncated\n            # at the beginning, which corresponds to the end of\n            # the original sequences.\n            truncating = 'pre'\n        else:\n            # Sequences that are too long should be truncated\n            # at the end.\n            truncating = 'post'\n\n        # The number of integer-tokens in each sequence.\n        self.num_tokens = [len(x) for x in self.tokens]\n\n        # Max number of tokens to use in all sequences.\n        # We will pad / truncate all sequences to this length.\n        # This is a compromise so we save a lot of memory and\n        # only have to truncate maybe 5% of all the sequences.\n        self.max_tokens = np.mean(self.num_tokens) \\\n                          + 2 * np.std(self.num_tokens)\n        self.max_tokens = int(self.max_tokens)\n\n        # Pad / truncate all token-sequences to the given length.\n        # This creates a 2-dim numpy matrix that is easier to use.\n        self.tokens_padded = pad_sequences(self.tokens,\n                                           maxlen=self.max_tokens,\n                                           padding=padding,\n                                           truncating=truncating)\n\n    def token_to_word(self, token):\n        \"\"\"Lookup a single word from an integer-token.\"\"\"\n\n        word = \" \" if token == 0 else self.index_to_word[token]\n        return word \n\n    def tokens_to_string(self, tokens):\n        \"\"\"Convert a list of integer-tokens to a string.\"\"\"\n\n        # Create a list of the individual words.\n        words = [self.index_to_word[token]\n                 for token in tokens\n                 if token != 0]\n        \n        # Concatenate the words to a single string\n        # with space between all the words.\n        text = \" \".join(words)\n\n        return text\n    \n    def text_to_tokens(self, text, reverse=False, padding=False):\n        \"\"\"\n        Convert a single text-string to tokens with optional\n        reversal and padding.\n        \"\"\"\n\n        # Convert to tokens. Note that we assume there is only\n        # a single text-string so we wrap it in a list.\n        tokens = self.texts_to_sequences([text])\n        tokens = np.array(tokens)\n\n        if reverse:\n            # Reverse the tokens.\n            tokens = np.flip(tokens, axis=1)\n\n            # Sequences that are too long should now be truncated\n            # at the beginning, which corresponds to the end of\n            # the original sequences.\n            truncating = 'pre'\n        else:\n            # Sequences that are too long should be truncated\n            # at the end.\n            truncating = 'post'\n\n        if padding:\n            # Pad and truncate sequences to the given length.\n            tokens = pad_sequences(tokens,\n                                   maxlen=self.max_tokens,\n                                   padding='pre',\n                                   truncating=truncating)\n\n        return tokens","metadata":{"execution":{"iopub.status.busy":"2024-11-02T16:51:48.350583Z","iopub.execute_input":"2024-11-02T16:51:48.351323Z","iopub.status.idle":"2024-11-02T16:51:48.519401Z","shell.execute_reply.started":"2024-11-02T16:51:48.351289Z","shell.execute_reply":"2024-11-02T16:51:48.518538Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\ntokenizer_src = TokenizerWrap(texts=vi_text,\n                              padding='pre',\n                              reverse=True,\n                              num_words=num_words\n                             )","metadata":{"execution":{"iopub.status.busy":"2024-11-02T16:51:50.302258Z","iopub.execute_input":"2024-11-02T16:51:50.303542Z","iopub.status.idle":"2024-11-02T16:55:14.893581Z","shell.execute_reply.started":"2024-11-02T16:51:50.303494Z","shell.execute_reply":"2024-11-02T16:55:14.892595Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\ntokenizer_des = TokenizerWrap(texts=eng_text,\n                              padding='post',\n                              reverse=False,\n                              num_words=num_words\n                             )","metadata":{"execution":{"iopub.status.busy":"2024-11-02T16:55:14.896801Z","iopub.execute_input":"2024-11-02T16:55:14.897097Z","iopub.status.idle":"2024-11-02T16:57:45.957924Z","shell.execute_reply.started":"2024-11-02T16:55:14.897068Z","shell.execute_reply":"2024-11-02T16:57:45.956912Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# II. Data Visualization","metadata":{}},{"cell_type":"markdown","source":"## 2.1 Create frequence dictionary","metadata":{}},{"cell_type":"code","source":"from collections import defaultdict\nfrom pprint import pprint\n\nnum_words = 10000\n\n# Khởi tạo đối tượng TokenizerWrap cho tiếng Anh\ntokenizer_eng = tokenizer_des\n# Khởi tạo đối tượng TokenizerWrap cho tiếng Việt\ntokenizer_vi = tokenizer_src","metadata":{"execution":{"iopub.status.busy":"2024-11-02T16:57:45.959821Z","iopub.execute_input":"2024-11-02T16:57:45.960137Z","iopub.status.idle":"2024-11-02T16:57:46.164552Z","shell.execute_reply.started":"2024-11-02T16:57:45.960106Z","shell.execute_reply":"2024-11-02T16:57:46.163538Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Bước 1: Chuyển đổi toàn bộ dữ liệu thành danh sách các token số nguyên\ntokens_eng = tokenizer_eng.tokens\ntokens_vi = tokenizer_vi.tokens\n\n# Bước 2: Tạo từ điển tần suất cho từng ngôn ngữ\ndef create_frequency_dict(tokens, tokenizer):\n    frequency_dict = defaultdict(int)\n    for sentence in tokens:\n        for token in sentence:\n            word = tokenizer.index_to_word[token]\n            frequency_dict[word] += 1\n    return frequency_dict\ndef print_samples(frequency_dict, lang):\n    samples = random.sample(list(frequency_dict.items()), 5)\n    print(f\"Samples from {lang} frequency dictionary:\")\n    for word, freq in samples:\n        print(f\"{word}: {freq}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-02T16:57:46.167436Z","iopub.execute_input":"2024-11-02T16:57:46.167845Z","iopub.status.idle":"2024-11-02T16:57:47.610124Z","shell.execute_reply.started":"2024-11-02T16:57:46.167805Z","shell.execute_reply":"2024-11-02T16:57:47.608940Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Tạo từ điển tần suất cho tiếng Anh và tiếng Việt\nfrequency_dict_eng = create_frequency_dict(tokens_eng, tokenizer_eng)\nfrequency_dict_vi = create_frequency_dict(tokens_vi, tokenizer_vi)\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-11-02T16:57:47.613790Z","iopub.execute_input":"2024-11-02T16:57:47.614120Z","iopub.status.idle":"2024-11-02T16:58:14.087529Z","shell.execute_reply.started":"2024-11-02T16:57:47.614075Z","shell.execute_reply":"2024-11-02T16:58:14.086556Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f'number of unique English word: {len(frequency_dict_eng)}')\nprint_samples(frequency_dict_eng, \"English\")\nprint(f'number of unique Viet Nam word: {len(frequency_dict_vi)}')\nprint_samples(frequency_dict_vi, \"Vietnamese\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T16:58:14.088756Z","iopub.execute_input":"2024-11-02T16:58:14.089085Z","iopub.status.idle":"2024-11-02T16:58:14.130170Z","shell.execute_reply.started":"2024-11-02T16:58:14.089054Z","shell.execute_reply":"2024-11-02T16:58:14.129125Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2.2 Word cloud visualizaiton","metadata":{}},{"cell_type":"code","source":"!pip install wordcloud matplotlib","metadata":{"execution":{"iopub.status.busy":"2024-11-02T16:58:14.131396Z","iopub.execute_input":"2024-11-02T16:58:14.131731Z","iopub.status.idle":"2024-11-02T16:58:25.769460Z","shell.execute_reply.started":"2024-11-02T16:58:14.131700Z","shell.execute_reply":"2024-11-02T16:58:25.768352Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\ndef create_wordcloud(frequency_dict, title):\n    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(frequency_dict)\n    \n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.title(title, fontsize=20)\n    plt.axis('off')\n    plt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-02T16:58:25.771398Z","iopub.execute_input":"2024-11-02T16:58:25.772364Z","iopub.status.idle":"2024-11-02T16:58:25.781599Z","shell.execute_reply.started":"2024-11-02T16:58:25.772320Z","shell.execute_reply":"2024-11-02T16:58:25.780622Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Hiển thị WordCloud cho tiếng Anh\ncreate_wordcloud(frequency_dict_eng, \"English WordCloud\")","metadata":{"execution":{"iopub.status.busy":"2024-11-02T16:58:25.782890Z","iopub.execute_input":"2024-11-02T16:58:25.783239Z","iopub.status.idle":"2024-11-02T16:58:26.671832Z","shell.execute_reply.started":"2024-11-02T16:58:25.783209Z","shell.execute_reply":"2024-11-02T16:58:26.670886Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Hiển thị WordCloud cho tiếng Việt\ncreate_wordcloud(frequency_dict_vi, \"Vietnamese WordCloud\")","metadata":{"execution":{"iopub.status.busy":"2024-11-02T16:58:26.677218Z","iopub.execute_input":"2024-11-02T16:58:26.677598Z","iopub.status.idle":"2024-11-02T16:58:27.614014Z","shell.execute_reply.started":"2024-11-02T16:58:26.677562Z","shell.execute_reply":"2024-11-02T16:58:27.613013Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2.3 Bar chart visualization","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom collections import defaultdict\n\n\ntop_words = sorted(frequency_dict_eng.items(), key=lambda x: x[1], reverse=True)[:20]\n\n# Tách từ và tần suất\nwords, counts = zip(*top_words)\n\n# Vẽ biểu đồ\nplt.figure(figsize=(10, 6))\nplt.bar(words, counts)\nplt.xlabel('Từ')\nplt.ylabel('Số lần xuất hiện')\nplt.title('Top 20 từ Tiếng anh xuất hiện nhiều nhất')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-02T16:58:27.615890Z","iopub.execute_input":"2024-11-02T16:58:27.616263Z","iopub.status.idle":"2024-11-02T16:58:27.965641Z","shell.execute_reply.started":"2024-11-02T16:58:27.616226Z","shell.execute_reply":"2024-11-02T16:58:27.964596Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom collections import defaultdict\n\n\ntop_words = sorted(frequency_dict_vi.items(), key=lambda x: x[1], reverse=True)[:20]\n\n# Tách từ và tần suất\nwords, counts = zip(*top_words)\n\n# Vẽ biểu đồ\nplt.figure(figsize=(10, 6))\nplt.bar(words, counts)\nplt.xlabel('Từ')\nplt.ylabel('Số lần xuất hiện')\nplt.title('Top 20 từ Tiếng việt xuất hiện nhiều nhất')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-02T16:58:27.966926Z","iopub.execute_input":"2024-11-02T16:58:27.967224Z","iopub.status.idle":"2024-11-02T16:58:28.316773Z","shell.execute_reply.started":"2024-11-02T16:58:27.967195Z","shell.execute_reply":"2024-11-02T16:58:28.315736Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# III. Model Definition","metadata":{}},{"cell_type":"code","source":"token_src = tokenizer_src.tokens_padded\ntoken_des = tokenizer_des.tokens_padded\n\nprint(token_src.shape)\nprint(token_des.shape)\ntoken_start = tokenizer_des.word_index[START.strip()]\ntoken_end = tokenizer_des.word_index[END.strip()]","metadata":{"execution":{"iopub.status.busy":"2024-11-02T16:58:28.318109Z","iopub.execute_input":"2024-11-02T16:58:28.318434Z","iopub.status.idle":"2024-11-02T16:58:28.324980Z","shell.execute_reply.started":"2024-11-02T16:58:28.318403Z","shell.execute_reply":"2024-11-02T16:58:28.323898Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoder_inp_data = token_src\ndecoder_inp_data = token_des[:, :-1]\ndecoder_out_data = token_des[:, 1:]","metadata":{"execution":{"iopub.status.busy":"2024-11-02T16:58:28.326236Z","iopub.execute_input":"2024-11-02T16:58:28.326630Z","iopub.status.idle":"2024-11-02T16:58:28.334662Z","shell.execute_reply.started":"2024-11-02T16:58:28.326581Z","shell.execute_reply":"2024-11-02T16:58:28.333630Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Encoder Model","metadata":{}},{"cell_type":"code","source":"# Glue all the encoder components together\ndef connect_encoder():\n    net = encoder_input\n    net = encoder_emb(net)\n    net = encoder_gru1(net)\n    net = encoder_gru2(net)\n    out = encoder_gru3(net)\n    \n    return out","metadata":{"execution":{"iopub.status.busy":"2024-11-02T16:58:28.335803Z","iopub.execute_input":"2024-11-02T16:58:28.336089Z","iopub.status.idle":"2024-11-02T16:58:28.343911Z","shell.execute_reply.started":"2024-11-02T16:58:28.336062Z","shell.execute_reply":"2024-11-02T16:58:28.343006Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Decoder Model","metadata":{}},{"cell_type":"code","source":"def connect_decoder(initial_state):    \n    # Start the decoder-network with its input-layer.\n    net = decoder_input\n\n    # Connect the embedding-layer.\n    net = decoder_emb(net)\n    \n    # Connect all the GRU-layers.\n    net = decoder_gru1(net, initial_state=initial_state)\n    net = decoder_gru2(net, initial_state=initial_state)\n    net = decoder_gru3(net, initial_state=initial_state)\n\n    # Connect the final dense layer that converts to\n    # one-hot encoded arrays.\n    decoder_output = decoder_dense(net)\n    \n    return decoder_output","metadata":{"execution":{"iopub.status.busy":"2024-11-02T16:58:28.345049Z","iopub.execute_input":"2024-11-02T16:58:28.345355Z","iopub.status.idle":"2024-11-02T16:58:28.354885Z","shell.execute_reply.started":"2024-11-02T16:58:28.345320Z","shell.execute_reply":"2024-11-02T16:58:28.354015Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2024-11-02T16:58:28.356053Z","iopub.execute_input":"2024-11-02T16:58:28.356381Z","iopub.status.idle":"2024-11-02T16:58:28.369024Z","shell.execute_reply.started":"2024-11-02T16:58:28.356343Z","shell.execute_reply":"2024-11-02T16:58:28.368068Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Connect all the models\nwith strategy.scope():\n    \n    embedding_size = 128\n    state_size = 512\n\n    encoder_input = Input(shape=(None,), name='encoder_input')\n    encoder_emb = Embedding(input_dim=num_words, output_dim=embedding_size, name='encoder_embedding')\n\n    encoder_gru1 = GRU(state_size, name='enc_gru1', return_sequences=True)\n    encoder_gru2 = GRU(state_size, name='enc_gru2', return_sequences=True)\n    encoder_gru3 = GRU(state_size, name='enc_gru3', return_sequences=False)\n    \n    encoder_op = connect_encoder()\n    \n    # Initial state placeholder takes a \"thought vector\" produced by the GRUs\n    # That's why it needs the inputs with \"state_size\" (which was used in GRU size)\n    decoder_initial_state = Input(shape=(state_size,), name='decoder_init_state')\n\n    # Decoder also needs an input, which is the basic input setence of the destination language\n    decoder_input = Input(shape=(None,), name='decoder_input')\n\n    # Have the decoder embedding\n    decoder_emb = Embedding(input_dim=num_words, output_dim=embedding_size, name='decoder_embedding')\n\n    # GRU arch similar to Encoder one with small changes\n    decoder_gru1 = GRU(state_size, name='dec_gru1', return_sequences=True)\n    decoder_gru2 = GRU(state_size, name='dec_gru2', return_sequences=True)\n    decoder_gru3 = GRU(state_size, name='dec_gru3', return_sequences=True)\n\n    # Final dense layer for prediction\n    decoder_dense = Dense(num_words, activation='softmax', name='decoder_output')\n    decoder_op = connect_decoder(encoder_op)\n    model_train = Model(inputs=[encoder_input, decoder_input],\n                        outputs=[decoder_op])\n    model_train.compile(optimizer=RMSprop(learning_rate=1e-3),\n                        loss='sparse_categorical_crossentropy')","metadata":{"execution":{"iopub.status.busy":"2024-11-02T16:58:28.370414Z","iopub.execute_input":"2024-11-02T16:58:28.370754Z","iopub.status.idle":"2024-11-02T16:58:30.216423Z","shell.execute_reply.started":"2024-11-02T16:58:28.370725Z","shell.execute_reply":"2024-11-02T16:58:30.215398Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**kiếm tra check point có tồn tại ( sử dụng để train model không liên tục, mỗi lần train 1 ít, khi train xong, lưu model vào checkpoint và sau load lại)**","metadata":{}},{"cell_type":"code","source":"path_checkpoint = '/kaggle/input/my-data/data_3M_train_checkpoint_41to50.keras'\npath_stored_checkpoint = \"data_3M_train_checkpoint_51.keras\"\n\n# Tạo callback để lưu checkpoint mới\ncallback_checkpoint = ModelCheckpoint(filepath=path_stored_checkpoint,\n                                      monitor='val_loss',\n                                      verbose=1,\n                                      save_weights_only=True,\n                                      save_best_only=True)","metadata":{"execution":{"iopub.status.busy":"2024-11-02T16:58:30.217646Z","iopub.execute_input":"2024-11-02T16:58:30.217922Z","iopub.status.idle":"2024-11-02T16:58:30.223473Z","shell.execute_reply.started":"2024-11-02T16:58:30.217894Z","shell.execute_reply":"2024-11-02T16:58:30.222539Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"try:\n    model_train.load_weights(path_checkpoint)\nexcept Exception as error:\n    print(\"Error trying to load checkpoint.\")\n    print(error)","metadata":{"execution":{"iopub.status.busy":"2024-11-02T16:58:30.224950Z","iopub.execute_input":"2024-11-02T16:58:30.225251Z","iopub.status.idle":"2024-11-02T16:58:30.769510Z","shell.execute_reply.started":"2024-11-02T16:58:30.225221Z","shell.execute_reply":"2024-11-02T16:58:30.768468Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x_data = {\n    \"encoder_input\": encoder_inp_data,\n    \"decoder_input\": decoder_inp_data\n}\n\ny_data = {\n    \"decoder_output\": decoder_out_data\n}","metadata":{"execution":{"iopub.status.busy":"2024-11-02T16:58:30.771055Z","iopub.execute_input":"2024-11-02T16:58:30.772041Z","iopub.status.idle":"2024-11-02T16:58:30.779918Z","shell.execute_reply.started":"2024-11-02T16:58:30.771998Z","shell.execute_reply":"2024-11-02T16:58:30.778845Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"callback_early_stopping = EarlyStopping(monitor='val_loss',\n                                        patience=5, verbose=1)\ncallbacks = [callback_early_stopping,\n             callback_checkpoint]","metadata":{"execution":{"iopub.status.busy":"2024-11-02T16:58:30.781245Z","iopub.execute_input":"2024-11-02T16:58:30.781649Z","iopub.status.idle":"2024-11-02T16:58:30.790443Z","shell.execute_reply.started":"2024-11-02T16:58:30.781603Z","shell.execute_reply":"2024-11-02T16:58:30.789420Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Skip tới bước demo nếu không cần chạy lại training**","metadata":{}},{"cell_type":"markdown","source":"## Training model","metadata":{}},{"cell_type":"code","source":"\n# with strategy.scope():\n#     model_train.fit(\n#         x=x_data,\n#         y=y_data,\n#         batch_size=512,\n#         epochs=10,\n#         callbacks=callbacks\n#     )\n    ","metadata":{"execution":{"iopub.status.busy":"2024-11-02T16:35:09.683792Z","iopub.status.idle":"2024-11-02T16:35:09.684154Z","shell.execute_reply.started":"2024-11-02T16:35:09.683975Z","shell.execute_reply":"2024-11-02T16:35:09.683992Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"epoch 1: ","metadata":{}},{"cell_type":"markdown","source":"#  IV. Evaluating with BLEU Score","metadata":{}},{"cell_type":"markdown","source":"## 1. Tải và tiền xử lý bộ test","metadata":{}},{"cell_type":"code","source":"\ntest_data = test_data[:10]\ntest_data = test_data[['en','vi']]\ntest_data.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-11-02T17:17:43.561229Z","iopub.execute_input":"2024-11-02T17:17:43.561669Z","iopub.status.idle":"2024-11-02T17:17:43.574421Z","shell.execute_reply.started":"2024-11-02T17:17:43.561631Z","shell.execute_reply":"2024-11-02T17:17:43.573220Z"},"trusted":true},"outputs":[{"execution_count":120,"output_type":"execute_result","data":{"text/plain":"                                                  en  \\\n0  In August 1764, Bertin permitted the export of...   \n1  Homeless women used to be invisible to me but ...   \n2  Pumping water site for artificial infiltration...   \n3                          I can't end it like this.   \n4                      Well, I guess it's done then.   \n5                     Okay. \"Release by\" \"Rotating\"-   \n6  Progress from 2000 onwards from IMF–OECD–FATF ...   \n7      I'm sorry, Adem, about the loss of your wife.   \n8  Here we have Qatar today, and there we have Ba...   \n9                      They seemed so... so human...   \n\n                                                  vi  \n0  Tháng 8 năm 1764, Bertin lại cho phép xuất khẩ...  \n1  Tôi từng không hề để ý đến những người phụ nữ ...  \n2        Bơm nước cho thấm nhân tạo ở quận Sojovice.  \n3   Tôi không thể kết thúc chuyện này như vậy.  \n4                     Vậy, ta đoán chúng ta đã xong.  \n5  Được rồi. \"Phóng thích bằng cách\" - \"Xoay\" - c...  \n6  Tiến bộ từ năm 2000 trở đi từ các sáng kiến IM...  \n7        Tôi rất tiếc, Adem, vì cái chết của vợ anh.  \n8  Ở đây chúng ta có Qatar hiện tại, và ở kia ta ...  \n9                   Chúng trông rất rất con người...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>en</th>\n      <th>vi</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>In August 1764, Bertin permitted the export of...</td>\n      <td>Tháng 8 năm 1764, Bertin lại cho phép xuất khẩ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Homeless women used to be invisible to me but ...</td>\n      <td>Tôi từng không hề để ý đến những người phụ nữ ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Pumping water site for artificial infiltration...</td>\n      <td>Bơm nước cho thấm nhân tạo ở quận Sojovice.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>I can't end it like this.</td>\n      <td>Tôi không thể kết thúc chuyện này như vậy.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Well, I guess it's done then.</td>\n      <td>Vậy, ta đoán chúng ta đã xong.</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Okay. \"Release by\" \"Rotating\"-</td>\n      <td>Được rồi. \"Phóng thích bằng cách\" - \"Xoay\" - c...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Progress from 2000 onwards from IMF–OECD–FATF ...</td>\n      <td>Tiến bộ từ năm 2000 trở đi từ các sáng kiến IM...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>I'm sorry, Adem, about the loss of your wife.</td>\n      <td>Tôi rất tiếc, Adem, vì cái chết của vợ anh.</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Here we have Qatar today, and there we have Ba...</td>\n      <td>Ở đây chúng ta có Qatar hiện tại, và ở kia ta ...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>They seemed so... so human...</td>\n      <td>Chúng trông rất rất con người...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":120},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom nltk.translate.bleu_score import corpus_bleu\n\ndef calculate_bleu_score(model, tokenizer_src, tokenizer_des, dataset, max_decoder_length=50):\n    # Khởi tạo danh sách để lưu trữ kết quả\n    references = []\n    hypotheses = []\n    \n    # Danh sách để lưu trữ các dự đoán\n    predictions = []\n\n    for index, row in dataset.iterrows():\n        # Lấy câu tiếng Việt và tiếng Anh\n        input_sentence = row['vi']\n        reference_sentence = row['en']\n\n        # Chuyển đổi câu thành tokens\n        input_tokens = tokenizer_src.text_to_tokens(input_sentence, reverse=True, padding=True)\n\n        # Tạo đầu vào cho decoder\n        decoder_input = np.zeros((1, max_decoder_length))\n        decoder_input[0, 0] = token_start\n\n        # Dự đoán câu dịch\n        for t in range(1, max_decoder_length):\n            output_tokens = model.predict([input_tokens, decoder_input])\n            sampled_token_index = np.argmax(output_tokens[0, t-1, :])\n            decoder_input[0, t] = sampled_token_index\n\n            # Dừng lại nếu gặp token <END>\n            if sampled_token_index == token_end:\n                break\n\n        # Chuyển đổi token thành chuỗi\n        translated_sentence = tokenizer_des.tokens_to_string(decoder_input[0])\n\n        # Thêm vào danh sách tham chiếu và giả thuyết\n        references.append([reference_sentence.split()])  # Tham chiếu cần là danh sách các từ\n        hypotheses.append(translated_sentence.split())\n        \n        # Lưu dự đoán\n        predictions.append(translated_sentence)\n\n    # Tính điểm BLEU\n    bleu_score = corpus_bleu(references, hypotheses)\n\n    # Tạo DataFrame mới với các cột 'vi', 'en', 'predict'\n    results_df = pd.DataFrame({\n        'vi': dataset['vi'],\n        'en': dataset['en'],\n        'predict': predictions\n    })\n\n    # Đổi thứ tự cột\n    results_df = results_df[['vi', 'en', 'predict']]\n    \n    return bleu_score, results_df\n\n# Ví dụ gọi hàm\n# dataset là một DataFrame chứa cột 'en' và 'vi'\nbleu_score, results_df = calculate_bleu_score(model, tokenizer_src, tokenizer_des, test_data)","metadata":{"execution":{"iopub.status.busy":"2024-11-02T17:19:54.908879Z","iopub.execute_input":"2024-11-02T17:19:54.909660Z","iopub.status.idle":"2024-11-02T17:20:03.182542Z","shell.execute_reply.started":"2024-11-02T17:19:54.909622Z","shell.execute_reply":"2024-11-02T17:20:03.181456Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \nThe hypothesis contains 0 counts of 4-gram overlaps.\nTherefore the BLEU score evaluates to 0, independently of\nhow many N-gram overlaps of lower order it contains.\nConsider using lower n-gram order or use SmoothingFunction()\n  warnings.warn(_msg)\n","output_type":"stream"}],"execution_count":127},{"cell_type":"markdown","source":"## 2. Dịch các câu và tính điểm BLEU trong bộ test","metadata":{}},{"cell_type":"code","source":"print(f\"BLEU score: {bleu_score}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-02T17:20:03.184274Z","iopub.execute_input":"2024-11-02T17:20:03.184614Z","iopub.status.idle":"2024-11-02T17:20:03.189734Z","shell.execute_reply.started":"2024-11-02T17:20:03.184582Z","shell.execute_reply":"2024-11-02T17:20:03.188721Z"},"trusted":true},"outputs":[{"name":"stdout","text":"BLEU score: 1.0957129348007817e-78\n","output_type":"stream"}],"execution_count":128},{"cell_type":"code","source":"results_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-11-02T17:20:03.191237Z","iopub.execute_input":"2024-11-02T17:20:03.191669Z","iopub.status.idle":"2024-11-02T17:20:03.206082Z","shell.execute_reply.started":"2024-11-02T17:20:03.191604Z","shell.execute_reply":"2024-11-02T17:20:03.204925Z"},"trusted":true},"outputs":[{"execution_count":129,"output_type":"execute_result","data":{"text/plain":"                                                  vi  \\\n0  Tháng 8 năm 1764, Bertin lại cho phép xuất khẩ...   \n1  Tôi từng không hề để ý đến những người phụ nữ ...   \n2        Bơm nước cho thấm nhân tạo ở quận Sojovice.   \n3   Tôi không thể kết thúc chuyện này như vậy.   \n4                     Vậy, ta đoán chúng ta đã xong.   \n5  Được rồi. \"Phóng thích bằng cách\" - \"Xoay\" - c...   \n6  Tiến bộ từ năm 2000 trở đi từ các sáng kiến IM...   \n7        Tôi rất tiếc, Adem, vì cái chết của vợ anh.   \n8  Ở đây chúng ta có Qatar hiện tại, và ở kia ta ...   \n9                   Chúng trông rất rất con người...   \n\n                                                  en  \\\n0  In August 1764, Bertin permitted the export of...   \n1  Homeless women used to be invisible to me but ...   \n2  Pumping water site for artificial infiltration...   \n3                          I can't end it like this.   \n4                      Well, I guess it's done then.   \n5                     Okay. \"Release by\" \"Rotating\"-   \n6  Progress from 2000 onwards from IMF–OECD–FATF ...   \n7      I'm sorry, Adem, about the loss of your wife.   \n8  Here we have Qatar today, and there we have Ba...   \n9                      They seemed so... so human...   \n\n                                             predict  \n0  ssss in august of the year allowed for grain e...  \n1  ssss i never noticed any of the women but now ...  \n2                        ssss artificial in the eeee  \n3                      ssss i can't finish this eeee  \n4                  ssss then i guess we're done eeee  \n5  ssss all right turn on by releasing something ...  \n6  ssss the from 2000 to the standard of initiati...  \n7                 ssss i'm sorry for your death eeee  \n8  ssss here we have qatar and here we are there ...  \n9  ssss they look very very very very very very v...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>vi</th>\n      <th>en</th>\n      <th>predict</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Tháng 8 năm 1764, Bertin lại cho phép xuất khẩ...</td>\n      <td>In August 1764, Bertin permitted the export of...</td>\n      <td>ssss in august of the year allowed for grain e...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Tôi từng không hề để ý đến những người phụ nữ ...</td>\n      <td>Homeless women used to be invisible to me but ...</td>\n      <td>ssss i never noticed any of the women but now ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Bơm nước cho thấm nhân tạo ở quận Sojovice.</td>\n      <td>Pumping water site for artificial infiltration...</td>\n      <td>ssss artificial in the eeee</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Tôi không thể kết thúc chuyện này như vậy.</td>\n      <td>I can't end it like this.</td>\n      <td>ssss i can't finish this eeee</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Vậy, ta đoán chúng ta đã xong.</td>\n      <td>Well, I guess it's done then.</td>\n      <td>ssss then i guess we're done eeee</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Được rồi. \"Phóng thích bằng cách\" - \"Xoay\" - c...</td>\n      <td>Okay. \"Release by\" \"Rotating\"-</td>\n      <td>ssss all right turn on by releasing something ...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Tiến bộ từ năm 2000 trở đi từ các sáng kiến IM...</td>\n      <td>Progress from 2000 onwards from IMF–OECD–FATF ...</td>\n      <td>ssss the from 2000 to the standard of initiati...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Tôi rất tiếc, Adem, vì cái chết của vợ anh.</td>\n      <td>I'm sorry, Adem, about the loss of your wife.</td>\n      <td>ssss i'm sorry for your death eeee</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Ở đây chúng ta có Qatar hiện tại, và ở kia ta ...</td>\n      <td>Here we have Qatar today, and there we have Ba...</td>\n      <td>ssss here we have qatar and here we are there ...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Chúng trông rất rất con người...</td>\n      <td>They seemed so... so human...</td>\n      <td>ssss they look very very very very very very v...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":129},{"cell_type":"markdown","source":"# V. Demo","metadata":{}},{"cell_type":"code","source":"import numpy as np\n# Ví dụ một câu tiếng Anh mới để dịch\nnew_sentences = [\n    \"Từ nơi đồng xanh thơm hương lúa, về nơi nhà cao xe giăng phố \",\n    \"Bạn khỏe không\",\n    \"bạn thấy thế nào\",\n    \"Học phát triển hệ thống thông minh\",\n    \"Chào mừng đến với bình nguyên vô tận\"\n]\nmodel = model_train\n# Chuyển đổi các câu thành tokens\ntokenized_inputs = [tokenizer_src.text_to_tokens(sentence, reverse=True, padding=True) for sentence in new_sentences]\n# Dự đoán cho từng câu\nmax_decoder_length = 20  # Thay đổi giá trị này nếu cần\ncount = 0\nfor input_tokens in tokenized_inputs:\n    # In cau goc\n    print(f\"Original sentence: {new_sentences[count]}\")\n    count +=1\n    # Tạo đầu vào cho decoder\n    decoder_input = np.zeros((1, max_decoder_length))  # Kích thước: (1, max_decoder_length)\n    decoder_input[0, 0] = token_start  # Bắt đầu bằng token <START>\n\n    # Lặp để dự đoán từng từ\n    for t in range(1, max_decoder_length):\n        output_tokens = model.predict([input_tokens, decoder_input])\n        sampled_token_index = np.argmax(output_tokens[0, t-1, :])\n        decoder_input[0, t] = sampled_token_index\n\n        # Dừng lại nếu gặp token <END>\n        if sampled_token_index == token_end:\n            break\n\n    # Chuyển đổi token thành chuỗi\n    translated_sentence = tokenizer_des.tokens_to_string(decoder_input[0])\n    print(f\"Translated: {translated_sentence}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-02T16:59:58.459846Z","iopub.execute_input":"2024-11-02T16:59:58.460438Z","iopub.status.idle":"2024-11-02T17:00:01.526549Z","shell.execute_reply.started":"2024-11-02T16:59:58.460407Z","shell.execute_reply":"2024-11-02T17:00:01.525515Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# IV. Store all file and load model","metadata":{}},{"cell_type":"markdown","source":"## 6.1 Store all needed file","metadata":{}},{"cell_type":"code","source":"import pickle\nfrom tensorflow.keras.models import load_model\n\n# Lưu mô hình đã huấn luyện\nmodel_train.save('model_train_50_epoch.h5')\n\n# Lưu tokenizer cho tiếng Việt\nwith open('tokenizer_src.pkl', 'wb') as f:\n    pickle.dump(tokenizer_src, f)\n\n# Lưu tokenizer cho tiếng Anh\nwith open('tokenizer_des.pkl', 'wb') as f:\n    pickle.dump(tokenizer_des, f)\n\n# Lưu các token đặc biệt\nspecial_tokens = {\n    'token_start': token_start,\n    'token_end': token_end\n}\nwith open('special_tokens.pkl', 'wb') as f:\n    pickle.dump(special_tokens, f)\n\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T16:35:09.697631Z","iopub.status.idle":"2024-11-02T16:35:09.697966Z","shell.execute_reply.started":"2024-11-02T16:35:09.697797Z","shell.execute_reply":"2024-11-02T16:35:09.697813Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6.2 Load the model and other component","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom tensorflow.keras.models import load_model\nimport pickle\n\n# Load the trained model\nmodel = load_model(\"/kaggle/working/model_train_50_epoch.h5\")\n\n# Load the tokenizers\nwith open('/kaggle/working/tokenizer_src.pkl', 'rb') as file:\n    tokenizer_src = pickle.load(file)\n\nwith open('/kaggle/working/tokenizer_des.pkl', 'rb') as file:\n    tokenizer_des = pickle.load(file)\nwith open('special_tokens.pkl', 'rb') as file:\n    special_tokens = pickle.load(file)\n\nSTART = special_tokens[\"token_start\"]\nEND = special_tokens[\"token_end\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T17:15:18.945739Z","iopub.execute_input":"2024-11-02T17:15:18.946135Z","iopub.status.idle":"2024-11-02T17:15:39.995093Z","shell.execute_reply.started":"2024-11-02T17:15:18.946099Z","shell.execute_reply":"2024-11-02T17:15:39.993932Z"}},"outputs":[],"execution_count":118},{"cell_type":"markdown","source":"## 6.3 Test with new data","metadata":{}},{"cell_type":"code","source":"new_sentences = [\n    \"Từ nơi đồng xanh thơm hương lúa, về nơi nhà cao xe giăng phố \",\n    \"Bạn khỏe không\",\n    \"bạn thấy thế nào\",\n    \"Học phát triển hệ thống thông minh\",\n    \"Chào mừng đến với bình nguyên vô tận\"\n]\n# Chuyển đổi các câu thành tokens\ntokenized_inputs = [tokenizer_src.text_to_tokens(sentence, reverse=True, padding=True) for sentence in new_sentences]\n# Dự đoán cho từng câu\nmax_decoder_length = 20  # Thay đổi giá trị này nếu cần\ncount = 0\nfor input_tokens in tokenized_inputs:\n    # In cau goc\n    print(f\"Original sentence: {new_sentences[count]}\")\n    count +=1\n    # Tạo đầu vào cho decoder\n    decoder_input = np.zeros((1, max_decoder_length))  # Kích thước: (1, max_decoder_length)\n    decoder_input[0, 0] = token_start  # Bắt đầu bằng token <START>\n\n    # Lặp để dự đoán từng từ\n    for t in range(1, max_decoder_length):\n        output_tokens = model.predict([input_tokens, decoder_input])\n        sampled_token_index = np.argmax(output_tokens[0, t-1, :])\n        decoder_input[0, t] = sampled_token_index\n\n        # Dừng lại nếu gặp token <END>\n        if sampled_token_index == token_end:\n            break\n\n    # Chuyển đổi token thành chuỗi\n    translated_sentence = tokenizer_des.tokens_to_string(decoder_input[0])\n    print(f\"Translated: {translated_sentence}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T17:15:42.732848Z","iopub.execute_input":"2024-11-02T17:15:42.733838Z","iopub.status.idle":"2024-11-02T17:15:46.336968Z","shell.execute_reply.started":"2024-11-02T17:15:42.733799Z","shell.execute_reply":"2024-11-02T17:15:46.335935Z"}},"outputs":[{"name":"stdout","text":"Original sentence: Từ nơi đồng xanh thơm hương lúa, về nơi nhà cao xe giăng phố \nTranslated: ssss from the green rice fields to the where the car is home eeee\nOriginal sentence: Bạn khỏe không\nTranslated: ssss how are you eeee\nOriginal sentence: bạn thấy thế nào\nTranslated: ssss how do you feel about it eeee\nOriginal sentence: Học phát triển hệ thống thông minh\nTranslated: ssss learning to develop intelligent systems eeee\nOriginal sentence: Chào mừng đến với bình nguyên vô tận\nTranslated: ssss welcome to the infinite eeee\n","output_type":"stream"}],"execution_count":119},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}