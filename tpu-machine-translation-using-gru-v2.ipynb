{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9777117,"sourceType":"datasetVersion","datasetId":5879313}],"dockerImageVersionId":30302,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Vietnamese to English MT\n","metadata":{}},{"cell_type":"markdown","source":"\n* I. Data Reading and Tokenization\n* II. Data Visualization\n* III. Model Defination\n* IV. Model Storage \n* V. Evaluating Elbow\n* VI. Demo","metadata":{}},{"cell_type":"markdown","source":"### Import library","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, GRU, Embedding\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-02T16:32:31.506594Z","iopub.execute_input":"2024-11-02T16:32:31.507297Z","iopub.status.idle":"2024-11-02T16:32:31.514000Z","shell.execute_reply.started":"2024-11-02T16:32:31.507260Z","shell.execute_reply":"2024-11-02T16:32:31.512919Z"},"trusted":true},"outputs":[],"execution_count":65},{"cell_type":"markdown","source":"# I. Data Reading and Tokenization","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/my-data/new_train_ds.csv\")\ndata = data.dropna()\ndata.head(10)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2024-11-02T16:32:31.518604Z","iopub.execute_input":"2024-11-02T16:32:31.518945Z","iopub.status.idle":"2024-11-02T16:32:45.833197Z","shell.execute_reply.started":"2024-11-02T16:32:31.518902Z","shell.execute_reply":"2024-11-02T16:32:45.832212Z"},"trusted":true},"outputs":[{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"                                                  en  \\\n0              - Sorry, that question's not on here.   \n1         He wants you to come with him immediately.   \n2               I thought we could use some company.   \n3  It was founded in 2008 by this anonymous progr...   \n4  With both of these methods, no two prints are ...   \n5  From these contexts was born an installation i...   \n6  I have lived to see something which I never ex...   \n7  It is the model for all future relationships w...   \n8                       Welcome him as your brother.   \n9  So biologists can make all the mutant fruit fl...   \n\n                                                  vi               source  \n0    - Xin lỗi, nhưng mà ở đây không có câu hỏi đấy.  OpenSubtitles v2018  \n1          Ông ấy muốn bố đi với ông ấy ngay lập tức  OpenSubtitles v2018  \n2  Tôi nghĩ chúng ta có thể muốn vài người bạn đồ...  OpenSubtitles v2018  \n3  Nó được sáng lập vào năm 2008 bởi một lập trìn...           TED2020 v1  \n4  Với cả hai phương pháp, không có hai bản in nà...           TED2020 v1  \n5  Từ những tình huống này một bố trí không gian ...           TED2020 v1  \n6  Ta đã sống để thấy điều ta không bao giờ mong ...  OpenSubtitles v2018  \n7  Đó là mô hình cho tất cả các mối quan hệ trong...           TED2020 v1  \n8               Chào mừng nó như anh em của các con.  OpenSubtitles v2018  \n9  Vậy các nhà sinh vật học có thể biến đổi gene ...           TED2020 v1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>en</th>\n      <th>vi</th>\n      <th>source</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>- Sorry, that question's not on here.</td>\n      <td>- Xin lỗi, nhưng mà ở đây không có câu hỏi đấy.</td>\n      <td>OpenSubtitles v2018</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>He wants you to come with him immediately.</td>\n      <td>Ông ấy muốn bố đi với ông ấy ngay lập tức</td>\n      <td>OpenSubtitles v2018</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I thought we could use some company.</td>\n      <td>Tôi nghĩ chúng ta có thể muốn vài người bạn đồ...</td>\n      <td>OpenSubtitles v2018</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>It was founded in 2008 by this anonymous progr...</td>\n      <td>Nó được sáng lập vào năm 2008 bởi một lập trìn...</td>\n      <td>TED2020 v1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>With both of these methods, no two prints are ...</td>\n      <td>Với cả hai phương pháp, không có hai bản in nà...</td>\n      <td>TED2020 v1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>From these contexts was born an installation i...</td>\n      <td>Từ những tình huống này một bố trí không gian ...</td>\n      <td>TED2020 v1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>I have lived to see something which I never ex...</td>\n      <td>Ta đã sống để thấy điều ta không bao giờ mong ...</td>\n      <td>OpenSubtitles v2018</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>It is the model for all future relationships w...</td>\n      <td>Đó là mô hình cho tất cả các mối quan hệ trong...</td>\n      <td>TED2020 v1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Welcome him as your brother.</td>\n      <td>Chào mừng nó như anh em của các con.</td>\n      <td>OpenSubtitles v2018</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>So biologists can make all the mutant fruit fl...</td>\n      <td>Vậy các nhà sinh vật học có thể biến đổi gene ...</td>\n      <td>TED2020 v1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":66},{"cell_type":"code","source":"data.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T16:32:45.834711Z","iopub.execute_input":"2024-11-02T16:32:45.834989Z","iopub.status.idle":"2024-11-02T16:32:45.841311Z","shell.execute_reply.started":"2024-11-02T16:32:45.834963Z","shell.execute_reply":"2024-11-02T16:32:45.840391Z"}},"outputs":[{"execution_count":67,"output_type":"execute_result","data":{"text/plain":"(2884451, 3)"},"metadata":{}}],"execution_count":67},{"cell_type":"code","source":"eng_text = data['en'].tolist()\nvi_text = data['vi'].tolist()\n\nprint(eng_text[0])\nprint(vi_text[0])\n","metadata":{"execution":{"iopub.status.busy":"2024-11-02T16:32:45.842559Z","iopub.execute_input":"2024-11-02T16:32:45.842854Z","iopub.status.idle":"2024-11-02T16:32:46.375331Z","shell.execute_reply.started":"2024-11-02T16:32:45.842826Z","shell.execute_reply":"2024-11-02T16:32:46.374332Z"},"trusted":true},"outputs":[{"name":"stdout","text":"- Sorry, that question's not on here.\n- Xin lỗi, nhưng mà ở đây không có câu hỏi đấy.\n","output_type":"stream"}],"execution_count":68},{"cell_type":"code","source":"num_words = 10000\nclass TokenizerWrap(Tokenizer):\n    \"\"\"Wrap the Tokenizer-class from Keras with more functionality.\"\"\"\n    \n    def __init__(self, texts, padding,\n                 reverse=False, num_words=None):\n        \"\"\"\n        :param texts: List of strings. This is the data-set.\n        :param padding: Either 'post' or 'pre' padding.\n        :param reverse: Boolean whether to reverse token-lists.\n        :param num_words: Max number of words to use.\n        \"\"\"\n\n        Tokenizer.__init__(self, num_words=num_words)\n\n        # Create the vocabulary from the texts.\n        self.fit_on_texts(texts)\n\n        # Create inverse lookup from integer-tokens to words.\n        self.index_to_word = dict(zip(self.word_index.values(),\n                                      self.word_index.keys()))\n\n        # Convert all texts to lists of integer-tokens.\n        # Note that the sequences may have different lengths.\n        self.tokens = self.texts_to_sequences(texts)\n\n        if reverse:\n            # Reverse the token-sequences.\n            self.tokens = [list(reversed(x)) for x in self.tokens]\n        \n            # Sequences that are too long should now be truncated\n            # at the beginning, which corresponds to the end of\n            # the original sequences.\n            truncating = 'pre'\n        else:\n            # Sequences that are too long should be truncated\n            # at the end.\n            truncating = 'post'\n\n        # The number of integer-tokens in each sequence.\n        self.num_tokens = [len(x) for x in self.tokens]\n\n        # Max number of tokens to use in all sequences.\n        # We will pad / truncate all sequences to this length.\n        # This is a compromise so we save a lot of memory and\n        # only have to truncate maybe 5% of all the sequences.\n        self.max_tokens = np.mean(self.num_tokens) \\\n                          + 2 * np.std(self.num_tokens)\n        self.max_tokens = int(self.max_tokens)\n\n        # Pad / truncate all token-sequences to the given length.\n        # This creates a 2-dim numpy matrix that is easier to use.\n        self.tokens_padded = pad_sequences(self.tokens,\n                                           maxlen=self.max_tokens,\n                                           padding=padding,\n                                           truncating=truncating)\n\n    def token_to_word(self, token):\n        \"\"\"Lookup a single word from an integer-token.\"\"\"\n\n        word = \" \" if token == 0 else self.index_to_word[token]\n        return word \n\n    def tokens_to_string(self, tokens):\n        \"\"\"Convert a list of integer-tokens to a string.\"\"\"\n\n        # Create a list of the individual words.\n        words = [self.index_to_word[token]\n                 for token in tokens\n                 if token != 0]\n        \n        # Concatenate the words to a single string\n        # with space between all the words.\n        text = \" \".join(words)\n\n        return text\n    \n    def text_to_tokens(self, text, reverse=False, padding=False):\n        \"\"\"\n        Convert a single text-string to tokens with optional\n        reversal and padding.\n        \"\"\"\n\n        # Convert to tokens. Note that we assume there is only\n        # a single text-string so we wrap it in a list.\n        tokens = self.texts_to_sequences([text])\n        tokens = np.array(tokens)\n\n        if reverse:\n            # Reverse the tokens.\n            tokens = np.flip(tokens, axis=1)\n\n            # Sequences that are too long should now be truncated\n            # at the beginning, which corresponds to the end of\n            # the original sequences.\n            truncating = 'pre'\n        else:\n            # Sequences that are too long should be truncated\n            # at the end.\n            truncating = 'post'\n\n        if padding:\n            # Pad and truncate sequences to the given length.\n            tokens = pad_sequences(tokens,\n                                   maxlen=self.max_tokens,\n                                   padding='pre',\n                                   truncating=truncating)\n\n        return tokens","metadata":{"execution":{"iopub.status.busy":"2024-11-02T16:32:46.377809Z","iopub.execute_input":"2024-11-02T16:32:46.378127Z","iopub.status.idle":"2024-11-02T16:32:46.549049Z","shell.execute_reply.started":"2024-11-02T16:32:46.378095Z","shell.execute_reply":"2024-11-02T16:32:46.547933Z"},"trusted":true},"outputs":[],"execution_count":69},{"cell_type":"code","source":"%%time\ntokenizer_src = TokenizerWrap(texts=vi_text,\n                              padding='pre',\n                              reverse=True,\n                              num_words=num_words\n                             )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\ntokenizer_des = TokenizerWrap(texts=eng_text,\n                              padding='post',\n                              reverse=False,\n                              num_words=num_words\n                             )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# II. Data Visualization","metadata":{}},{"cell_type":"markdown","source":"## 2.1 Create frequence dictionary","metadata":{}},{"cell_type":"code","source":"from collections import defaultdict\nfrom pprint import pprint\n\nnum_words = 10000\n\n# Khởi tạo đối tượng TokenizerWrap cho tiếng Anh\ntokenizer_eng = tokenizer_des\n# Khởi tạo đối tượng TokenizerWrap cho tiếng Việt\ntokenizer_vi = tokenizer_src","metadata":{"execution":{"iopub.status.busy":"2024-11-02T16:34:57.048582Z","iopub.execute_input":"2024-11-02T16:34:57.048955Z","iopub.status.idle":"2024-11-02T16:34:57.054606Z","shell.execute_reply.started":"2024-11-02T16:34:57.048902Z","shell.execute_reply":"2024-11-02T16:34:57.053541Z"},"trusted":true},"outputs":[],"execution_count":72},{"cell_type":"code","source":"# Bước 1: Chuyển đổi toàn bộ dữ liệu thành danh sách các token số nguyên\ntokens_eng = tokenizer_eng.tokens\ntokens_vi = tokenizer_vi.tokens\n\n# Bước 2: Tạo từ điển tần suất cho từng ngôn ngữ\ndef create_frequency_dict(tokens, tokenizer):\n    frequency_dict = defaultdict(int)\n    for sentence in tokens:\n        for token in sentence:\n            word = tokenizer.index_to_word[token]\n            frequency_dict[word] += 1\n    return frequency_dict\ndef print_samples(frequency_dict, lang):\n    samples = random.sample(list(frequency_dict.items()), 5)\n    print(f\"Samples from {lang} frequency dictionary:\")\n    for word, freq in samples:\n        print(f\"{word}: {freq}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-02T16:34:57.056152Z","iopub.execute_input":"2024-11-02T16:34:57.056435Z","iopub.status.idle":"2024-11-02T16:34:57.068164Z","shell.execute_reply.started":"2024-11-02T16:34:57.056407Z","shell.execute_reply":"2024-11-02T16:34:57.067237Z"},"trusted":true},"outputs":[],"execution_count":73},{"cell_type":"code","source":"# Tạo từ điển tần suất cho tiếng Anh và tiếng Việt\nfrequency_dict_eng = create_frequency_dict(tokens_eng, tokenizer_eng)\nfrequency_dict_vi = create_frequency_dict(tokens_vi, tokenizer_vi)\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-11-02T16:34:57.069430Z","iopub.execute_input":"2024-11-02T16:34:57.069810Z","iopub.status.idle":"2024-11-02T16:35:09.649596Z","shell.execute_reply.started":"2024-11-02T16:34:57.069777Z","shell.execute_reply":"2024-11-02T16:35:09.646209Z"},"trusted":true},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/308101214.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Tạo từ điển tần suất cho tiếng Anh và tiếng Việt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfrequency_dict_eng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_frequency_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens_eng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_eng\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfrequency_dict_vi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_frequency_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens_vi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_vi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_27/2597929137.py\u001b[0m in \u001b[0;36mcreate_frequency_dict\u001b[0;34m(tokens, tokenizer)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_to_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mfrequency_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrequency_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":74},{"cell_type":"code","source":"print(f'number of unique English word: {len(frequency_dict_eng)}')\nprint_samples(frequency_dict_eng, \"English\")\nprint(f'number of unique Viet Nam word: {len(frequency_dict_vi)}')\nprint_samples(frequency_dict_vi, \"Vietnamese\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T16:35:09.650620Z","iopub.status.idle":"2024-11-02T16:35:09.651020Z","shell.execute_reply.started":"2024-11-02T16:35:09.650829Z","shell.execute_reply":"2024-11-02T16:35:09.650848Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2.2 Word cloud visualizaiton","metadata":{}},{"cell_type":"code","source":"!pip install wordcloud matplotlib","metadata":{"execution":{"iopub.status.busy":"2024-11-02T16:35:09.652666Z","iopub.status.idle":"2024-11-02T16:35:09.653012Z","shell.execute_reply.started":"2024-11-02T16:35:09.652841Z","shell.execute_reply":"2024-11-02T16:35:09.652857Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\ndef create_wordcloud(frequency_dict, title):\n    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(frequency_dict)\n    \n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.title(title, fontsize=20)\n    plt.axis('off')\n    plt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-02T16:35:09.654379Z","iopub.status.idle":"2024-11-02T16:35:09.655261Z","shell.execute_reply.started":"2024-11-02T16:35:09.655055Z","shell.execute_reply":"2024-11-02T16:35:09.655081Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Hiển thị WordCloud cho tiếng Anh\ncreate_wordcloud(frequency_dict_eng, \"English WordCloud\")","metadata":{"execution":{"iopub.status.busy":"2024-11-02T16:35:09.656729Z","iopub.status.idle":"2024-11-02T16:35:09.657104Z","shell.execute_reply.started":"2024-11-02T16:35:09.656920Z","shell.execute_reply":"2024-11-02T16:35:09.656939Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Hiển thị WordCloud cho tiếng Việt\ncreate_wordcloud(frequency_dict_vi, \"Vietnamese WordCloud\")","metadata":{"execution":{"iopub.status.busy":"2024-11-02T16:35:09.658129Z","iopub.status.idle":"2024-11-02T16:35:09.658464Z","shell.execute_reply.started":"2024-11-02T16:35:09.658289Z","shell.execute_reply":"2024-11-02T16:35:09.658305Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2.3 Bar chart visualization","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom collections import defaultdict\n\n\ntop_words = sorted(frequency_dict_eng.items(), key=lambda x: x[1], reverse=True)[:20]\n\n# Tách từ và tần suất\nwords, counts = zip(*top_words)\n\n# Vẽ biểu đồ\nplt.figure(figsize=(10, 6))\nplt.bar(words, counts)\nplt.xlabel('Từ')\nplt.ylabel('Số lần xuất hiện')\nplt.title('Top 20 từ Tiếng anh xuất hiện nhiều nhất')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-02T16:35:09.659750Z","iopub.status.idle":"2024-11-02T16:35:09.660110Z","shell.execute_reply.started":"2024-11-02T16:35:09.659932Z","shell.execute_reply":"2024-11-02T16:35:09.659949Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom collections import defaultdict\n\n\ntop_words = sorted(frequency_dict_vi.items(), key=lambda x: x[1], reverse=True)[:20]\n\n# Tách từ và tần suất\nwords, counts = zip(*top_words)\n\n# Vẽ biểu đồ\nplt.figure(figsize=(10, 6))\nplt.bar(words, counts)\nplt.xlabel('Từ')\nplt.ylabel('Số lần xuất hiện')\nplt.title('Top 20 từ Tiếng việt xuất hiện nhiều nhất')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-02T16:35:09.661582Z","iopub.status.idle":"2024-11-02T16:35:09.661917Z","shell.execute_reply.started":"2024-11-02T16:35:09.661746Z","shell.execute_reply":"2024-11-02T16:35:09.661763Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# III. Model Definition","metadata":{}},{"cell_type":"code","source":"token_src = tokenizer_src.tokens_padded\ntoken_des = tokenizer_des.tokens_padded\n\nprint(token_src.shape)\nprint(token_des.shape)\ntoken_start = tokenizer_des.word_index[START.strip()]\ntoken_end = tokenizer_des.word_index[END.strip()]","metadata":{"execution":{"iopub.status.busy":"2024-11-02T16:35:09.663668Z","iopub.status.idle":"2024-11-02T16:35:09.664005Z","shell.execute_reply.started":"2024-11-02T16:35:09.663836Z","shell.execute_reply":"2024-11-02T16:35:09.663851Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoder_inp_data = token_src\ndecoder_inp_data = token_des[:, :-1]\ndecoder_out_data = token_des[:, 1:]","metadata":{"execution":{"iopub.status.busy":"2024-11-02T16:35:09.665168Z","iopub.status.idle":"2024-11-02T16:35:09.665675Z","shell.execute_reply.started":"2024-11-02T16:35:09.665395Z","shell.execute_reply":"2024-11-02T16:35:09.665417Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Encoder Model","metadata":{}},{"cell_type":"code","source":"# Glue all the encoder components together\ndef connect_encoder():\n    net = encoder_input\n    net = encoder_emb(net)\n    net = encoder_gru1(net)\n    net = encoder_gru2(net)\n    out = encoder_gru3(net)\n    \n    return out","metadata":{"execution":{"iopub.status.busy":"2024-11-02T16:35:09.667295Z","iopub.status.idle":"2024-11-02T16:35:09.667678Z","shell.execute_reply.started":"2024-11-02T16:35:09.667468Z","shell.execute_reply":"2024-11-02T16:35:09.667501Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Decoder Model","metadata":{}},{"cell_type":"code","source":"def connect_decoder(initial_state):    \n    # Start the decoder-network with its input-layer.\n    net = decoder_input\n\n    # Connect the embedding-layer.\n    net = decoder_emb(net)\n    \n    # Connect all the GRU-layers.\n    net = decoder_gru1(net, initial_state=initial_state)\n    net = decoder_gru2(net, initial_state=initial_state)\n    net = decoder_gru3(net, initial_state=initial_state)\n\n    # Connect the final dense layer that converts to\n    # one-hot encoded arrays.\n    decoder_output = decoder_dense(net)\n    \n    return decoder_output","metadata":{"execution":{"iopub.status.busy":"2024-11-02T16:35:09.669571Z","iopub.status.idle":"2024-11-02T16:35:09.669940Z","shell.execute_reply.started":"2024-11-02T16:35:09.669759Z","shell.execute_reply":"2024-11-02T16:35:09.669777Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2024-11-02T16:35:09.671960Z","iopub.status.idle":"2024-11-02T16:35:09.672334Z","shell.execute_reply.started":"2024-11-02T16:35:09.672150Z","shell.execute_reply":"2024-11-02T16:35:09.672168Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Connect all the models\nwith strategy.scope():\n    \n    embedding_size = 128\n    state_size = 512\n\n    encoder_input = Input(shape=(None,), name='encoder_input')\n    encoder_emb = Embedding(input_dim=num_words, output_dim=embedding_size, name='encoder_embedding')\n\n    encoder_gru1 = GRU(state_size, name='enc_gru1', return_sequences=True)\n    encoder_gru2 = GRU(state_size, name='enc_gru2', return_sequences=True)\n    encoder_gru3 = GRU(state_size, name='enc_gru3', return_sequences=False)\n    \n    encoder_op = connect_encoder()\n    \n    # Initial state placeholder takes a \"thought vector\" produced by the GRUs\n    # That's why it needs the inputs with \"state_size\" (which was used in GRU size)\n    decoder_initial_state = Input(shape=(state_size,), name='decoder_init_state')\n\n    # Decoder also needs an input, which is the basic input setence of the destination language\n    decoder_input = Input(shape=(None,), name='decoder_input')\n\n    # Have the decoder embedding\n    decoder_emb = Embedding(input_dim=num_words, output_dim=embedding_size, name='decoder_embedding')\n\n    # GRU arch similar to Encoder one with small changes\n    decoder_gru1 = GRU(state_size, name='dec_gru1', return_sequences=True)\n    decoder_gru2 = GRU(state_size, name='dec_gru2', return_sequences=True)\n    decoder_gru3 = GRU(state_size, name='dec_gru3', return_sequences=True)\n\n    # Final dense layer for prediction\n    decoder_dense = Dense(num_words, activation='softmax', name='decoder_output')\n    decoder_op = connect_decoder(encoder_op)\n    model_train = Model(inputs=[encoder_input, decoder_input],\n                        outputs=[decoder_op])\n    model_train.compile(optimizer=RMSprop(learning_rate=1e-3),\n                        loss='sparse_categorical_crossentropy')","metadata":{"execution":{"iopub.status.busy":"2024-11-02T16:35:09.674109Z","iopub.status.idle":"2024-11-02T16:35:09.674618Z","shell.execute_reply.started":"2024-11-02T16:35:09.674334Z","shell.execute_reply":"2024-11-02T16:35:09.674357Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**kiếm tra check point có tồn tại ( sử dụng để train model không liên tục, mỗi lần train 1 ít, khi train xong, lưu model vào checkpoint và sau load lại)**","metadata":{}},{"cell_type":"code","source":"path_checkpoint = '/kaggle/input/my-data/data_3M_train_checkpoint_41to50.keras'\npath_stored_checkpoint = \"data_3M_train_checkpoint_51.keras\"\n\n# Tạo callback để lưu checkpoint mới\ncallback_checkpoint = ModelCheckpoint(filepath=path_stored_checkpoint,\n                                      monitor='val_loss',\n                                      verbose=1,\n                                      save_weights_only=True,\n                                      save_best_only=True)","metadata":{"execution":{"iopub.status.busy":"2024-11-02T16:35:09.675820Z","iopub.status.idle":"2024-11-02T16:35:09.676291Z","shell.execute_reply.started":"2024-11-02T16:35:09.676046Z","shell.execute_reply":"2024-11-02T16:35:09.676068Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"try:\n    model_train.load_weights(path_checkpoint)\nexcept Exception as error:\n    print(\"Error trying to load checkpoint.\")\n    print(error)","metadata":{"execution":{"iopub.status.busy":"2024-11-02T16:35:09.678137Z","iopub.status.idle":"2024-11-02T16:35:09.678509Z","shell.execute_reply.started":"2024-11-02T16:35:09.678304Z","shell.execute_reply":"2024-11-02T16:35:09.678320Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x_data = {\n    \"encoder_input\": encoder_inp_data,\n    \"decoder_input\": decoder_inp_data\n}\n\ny_data = {\n    \"decoder_output\": decoder_out_data\n}","metadata":{"execution":{"iopub.status.busy":"2024-11-02T16:35:09.680297Z","iopub.status.idle":"2024-11-02T16:35:09.680688Z","shell.execute_reply.started":"2024-11-02T16:35:09.680463Z","shell.execute_reply":"2024-11-02T16:35:09.680502Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"callback_early_stopping = EarlyStopping(monitor='val_loss',\n                                        patience=5, verbose=1)\ncallbacks = [callback_early_stopping,\n             callback_checkpoint]","metadata":{"execution":{"iopub.status.busy":"2024-11-02T16:35:09.682166Z","iopub.status.idle":"2024-11-02T16:35:09.682560Z","shell.execute_reply.started":"2024-11-02T16:35:09.682348Z","shell.execute_reply":"2024-11-02T16:35:09.682366Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Skip tới bước demo nếu không cần chạy lại training**","metadata":{}},{"cell_type":"markdown","source":"## Training model","metadata":{}},{"cell_type":"code","source":"\n# with strategy.scope():\n#     model_train.fit(\n#         x=x_data,\n#         y=y_data,\n#         batch_size=512,\n#         epochs=10,\n#         callbacks=callbacks\n#     )\n    ","metadata":{"execution":{"iopub.status.busy":"2024-11-02T16:35:09.683792Z","iopub.status.idle":"2024-11-02T16:35:09.684154Z","shell.execute_reply.started":"2024-11-02T16:35:09.683975Z","shell.execute_reply":"2024-11-02T16:35:09.683992Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"epoch 1: ","metadata":{}},{"cell_type":"markdown","source":"#  IV. Evaluating with BLEU Score","metadata":{}},{"cell_type":"markdown","source":"## 1. Tải và tiền xử lý bộ test","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Load test data\nmodel = model_train\ntest_data = pd.read_csv(\"/kaggle/input/my-data/test_ds.csv\")\ntest_data = test_data.dropna()\ntest_data = test_data[:100]\ntest_data = test_data[['en','vi']]\ntest_data.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-11-02T16:35:09.686101Z","iopub.status.idle":"2024-11-02T16:35:09.686439Z","shell.execute_reply.started":"2024-11-02T16:35:09.686268Z","shell.execute_reply":"2024-11-02T16:35:09.686283Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import numpy as np\n# import pandas as pd\n# from nltk.translate.bleu_score import corpus_bleu\n\n# def calculate_bleu_score(model, tokenizer_src, tokenizer_des, dataset, max_decoder_length=50):\n#     # Khởi tạo danh sách để lưu trữ kết quả\n#     references = []\n#     hypotheses = []\n    \n#     # Danh sách để lưu trữ các dự đoán\n#     predictions = []\n\n#     for index, row in dataset.iterrows():\n#         # Lấy câu tiếng Việt và tiếng Anh\n#         input_sentence = row['vi']\n#         reference_sentence = row['en'].lower()\n\n#         # Chuyển đổi câu thành tokens\n#         input_tokens = tokenizer_src.text_to_tokens(input_sentence, reverse=True, padding=True)\n\n#         # Tạo đầu vào cho decoder\n#         decoder_input = np.zeros((1, max_decoder_length))\n#         decoder_input[0, 0] = token_start\n\n#         # Dự đoán câu dịch\n#         for t in range(1, max_decoder_length): # dự đoán từ từ 1 tới max\n#             # ouput của đoạn này là từ tiềm năng cho vị trí t.\n            \n#             # dựa vào input token và decoder để tính xác suất các từ là từ thứ t.\n#             output_tokens = model.predict([input_tokens, decoder_input])\n#             # trong các từ đó lấy từ có xác suất cao nhât \n#             # chú ý index \n#             sampled_token_index = np.argmax(output_tokens[0, t-1, :])\n#             decoder_input[0, t] = sampled_token_index\n\n#             # Dừng lại nếu gặp token <END>\n#             # loại bỏ (start - end) token\n#             if sampled_token_index == token_end:\n#                 break\n\n#         # Chuyển đổi token thành chuỗi \n#         translated_sentence = tokenizer_des.tokens_to_string(decoder_input[0])\n\n#         # Thêm vào danh sách tham chiếu và giả thuyết\n        \n#         references.append([reference_sentence.split()])  # Tham chiếu cần là danh sách các từ\n#         hypotheses.append(translated_sentence.split()[1:-1])\n        \n#         # Lưu dự đoán\n#         predictions.append(translated_sentence)\n\n#     # Tính điểm BLEU\n#     bleu_score = corpus_bleu(references, hypotheses)\n\n#     # Tạo DataFrame mới với các cột 'vi', 'en', 'predict'\n#     results_df = pd.DataFrame({\n#         'vi': dataset['vi'],\n#         'en': dataset['en'],\n#         'predict': predictions\n#     })\n\n#     # Đổi thứ tự cột\n#     results_df = results_df[['vi', 'en', 'predict']]\n    \n#     return bleu_score, results_df\n\n# # Ví dụ gọi hàm\n# # dataset là một DataFrame chứa cột 'en' và 'vi'\n# bleu_score, results_df = calculate_bleu_score(model, tokenizer_src, tokenizer_des, test_data)","metadata":{"execution":{"iopub.status.busy":"2024-11-02T16:35:09.688264Z","iopub.status.idle":"2024-11-02T16:35:09.688654Z","shell.execute_reply.started":"2024-11-02T16:35:09.688433Z","shell.execute_reply":"2024-11-02T16:35:09.688449Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Dịch các câu và tính điểm BLEU trong bộ test","metadata":{"jp-MarkdownHeadingCollapsed":true}},{"cell_type":"code","source":"# print(f\"BLEU score: {bleu_score}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-02T16:35:09.690414Z","iopub.status.idle":"2024-11-02T16:35:09.690798Z","shell.execute_reply.started":"2024-11-02T16:35:09.690618Z","shell.execute_reply":"2024-11-02T16:35:09.690635Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# results_df[15:20]","metadata":{"execution":{"iopub.status.busy":"2024-11-02T16:35:09.692407Z","iopub.status.idle":"2024-11-02T16:35:09.692813Z","shell.execute_reply.started":"2024-11-02T16:35:09.692627Z","shell.execute_reply":"2024-11-02T16:35:09.692645Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# results_df['vi'][15]\n# results_df['en'][15]\n# results_df['predict'][15]","metadata":{"execution":{"iopub.status.busy":"2024-11-02T16:35:09.694206Z","iopub.status.idle":"2024-11-02T16:35:09.694614Z","shell.execute_reply.started":"2024-11-02T16:35:09.694389Z","shell.execute_reply":"2024-11-02T16:35:09.694406Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# V. Demo","metadata":{}},{"cell_type":"code","source":"import numpy as np\n# Ví dụ một câu tiếng Anh mới để dịch\nnew_sentences = [\n    \"Từ nơi đồng xanh thơm hương lúa, về nơi nhà cao xe giăng phố \",\n    \"Bạn khỏe không\",\n    \"bạn thấy thế nào\",\n    \"Học phát triển hệ thống thông minh\",\n    \"Chào mừng đến với bình nguyên vô tận\"\n]\nmodel = model_train\n# Chuyển đổi các câu thành tokens\ntokenized_inputs = [tokenizer_src.text_to_tokens(sentence, reverse=True, padding=True) for sentence in new_sentences]\n# Dự đoán cho từng câu\nmax_decoder_length = 20  # Thay đổi giá trị này nếu cần\ncount = 0\nfor input_tokens in tokenized_inputs:\n    # In cau goc\n    print(f\"Original sentence: {new_sentences[count]}\")\n    count +=1\n    # Tạo đầu vào cho decoder\n    decoder_input = np.zeros((1, max_decoder_length))  # Kích thước: (1, max_decoder_length)\n    decoder_input[0, 0] = token_start  # Bắt đầu bằng token <START>\n\n    # Lặp để dự đoán từng từ\n    for t in range(1, max_decoder_length):\n        output_tokens = model.predict([input_tokens, decoder_input])\n        sampled_token_index = np.argmax(output_tokens[0, t-1, :])\n        decoder_input[0, t] = sampled_token_index\n\n        # Dừng lại nếu gặp token <END>\n        if sampled_token_index == token_end:\n            break\n\n    # Chuyển đổi token thành chuỗi\n    translated_sentence = tokenizer_des.tokens_to_string(decoder_input[0])\n    print(f\"Translated: {translated_sentence}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-02T16:35:09.695938Z","iopub.status.idle":"2024-11-02T16:35:09.696299Z","shell.execute_reply.started":"2024-11-02T16:35:09.696119Z","shell.execute_reply":"2024-11-02T16:35:09.696136Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# IV. Store all file and load model","metadata":{}},{"cell_type":"markdown","source":"## 6.1 Store all needed file","metadata":{}},{"cell_type":"code","source":"import pickle\nfrom tensorflow.keras.models import load_model\n\n# Lưu mô hình đã huấn luyện\nmodel_train.save('model_train_50_epoch.h5')\n\n# Lưu tokenizer cho tiếng Việt\nwith open('tokenizer_src.pkl', 'wb') as f:\n    pickle.dump(tokenizer_src, f)\n\n# Lưu tokenizer cho tiếng Anh\nwith open('tokenizer_des.pkl', 'wb') as f:\n    pickle.dump(tokenizer_des, f)\n\n# Lưu các token đặc biệt\nspecial_tokens = {\n    'token_start': token_start,\n    'token_end': token_end\n}\nwith open('special_tokens.pkl', 'wb') as f:\n    pickle.dump(special_tokens, f)\n\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T16:35:09.697631Z","iopub.status.idle":"2024-11-02T16:35:09.697966Z","shell.execute_reply.started":"2024-11-02T16:35:09.697797Z","shell.execute_reply":"2024-11-02T16:35:09.697813Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom tensorflow.keras.models import load_model\nimport pickle\n\n# Load the trained model\nmodel = load_model(\"/kaggle/working/model_train_50_epoch.h5\")\n\n# Load the tokenizers\nwith open('/kaggle/working/tokenizer_src.pkl', 'rb') as file:\n    tokenizer_src = pickle.load(file)\n\nwith open('/kaggle/working/tokenizer_des.pkl', 'rb') as file:\n    tokenizer_des = pickle.load(file)\nwith open('special_tokens.pkl', 'rb') as file:\n    special_tokens = pickle.load(file)\n\nSTART = special_tokens[\"token_start\"]\nEND = special_tokens[\"token_end\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T16:42:58.421679Z","iopub.execute_input":"2024-11-02T16:42:58.422053Z","iopub.status.idle":"2024-11-02T16:43:18.928377Z","shell.execute_reply.started":"2024-11-02T16:42:58.422020Z","shell.execute_reply":"2024-11-02T16:43:18.927532Z"}},"outputs":[],"execution_count":78},{"cell_type":"code","source":"new_sentences = [\n    \"Từ nơi đồng xanh thơm hương lúa, về nơi nhà cao xe giăng phố \",\n    \"Bạn khỏe không\",\n    \"bạn thấy thế nào\",\n    \"Học phát triển hệ thống thông minh\",\n    \"Chào mừng đến với bình nguyên vô tận\"\n]\n# Chuyển đổi các câu thành tokens\ntokenized_inputs = [tokenizer_src.text_to_tokens(sentence, reverse=True, padding=True) for sentence in new_sentences]\n# Dự đoán cho từng câu\nmax_decoder_length = 20  # Thay đổi giá trị này nếu cần\ncount = 0\nfor input_tokens in tokenized_inputs:\n    # In cau goc\n    print(f\"Original sentence: {new_sentences[count]}\")\n    count +=1\n    # Tạo đầu vào cho decoder\n    decoder_input = np.zeros((1, max_decoder_length))  # Kích thước: (1, max_decoder_length)\n    decoder_input[0, 0] = token_start  # Bắt đầu bằng token <START>\n\n    # Lặp để dự đoán từng từ\n    for t in range(1, max_decoder_length):\n        output_tokens = model.predict([input_tokens, decoder_input])\n        sampled_token_index = np.argmax(output_tokens[0, t-1, :])\n        decoder_input[0, t] = sampled_token_index\n\n        # Dừng lại nếu gặp token <END>\n        if sampled_token_index == token_end:\n            break\n\n    # Chuyển đổi token thành chuỗi\n    translated_sentence = tokenizer_des.tokens_to_string(decoder_input[0])\n    print(f\"Translated: {translated_sentence}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T16:43:23.382629Z","iopub.execute_input":"2024-11-02T16:43:23.383473Z","iopub.status.idle":"2024-11-02T16:43:26.836426Z","shell.execute_reply.started":"2024-11-02T16:43:23.383437Z","shell.execute_reply":"2024-11-02T16:43:26.835388Z"}},"outputs":[{"name":"stdout","text":"Original sentence: Từ nơi đồng xanh thơm hương lúa, về nơi nhà cao xe giăng phố \nTranslated: ssss from the green rice fields to the where the car is home eeee\nOriginal sentence: Bạn khỏe không\nTranslated: ssss how are you eeee\nOriginal sentence: bạn thấy thế nào\nTranslated: ssss how do you feel about it eeee\nOriginal sentence: Học phát triển hệ thống thông minh\nTranslated: ssss learning to develop intelligent systems eeee\nOriginal sentence: Chào mừng đến với bình nguyên vô tận\nTranslated: ssss welcome to the infinite eeee\n","output_type":"stream"}],"execution_count":79},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}