{"cells":[{"cell_type":"markdown","metadata":{},"source":["<p style=\"color:grey\">This work was largely inspired by Hvass Labs work on the same (Thank you!)</p>"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-10-08T09:02:34.993096Z","iopub.status.busy":"2024-10-08T09:02:34.992097Z","iopub.status.idle":"2024-10-08T09:02:46.128424Z","shell.execute_reply":"2024-10-08T09:02:46.127465Z","shell.execute_reply.started":"2024-10-08T09:02:34.993001Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","import tensorflow as tf\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Dense, GRU, Embedding\n","from tensorflow.keras.optimizers import RMSprop\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T09:03:34.063662Z","iopub.status.busy":"2024-10-08T09:03:34.062410Z","iopub.status.idle":"2024-10-08T09:03:34.083881Z","shell.execute_reply":"2024-10-08T09:03:34.082868Z","shell.execute_reply.started":"2024-10-08T09:03:34.063621Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["REPLICAS:  1\n"]}],"source":["try:\n","    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n","    print('Running on TPU ', tpu.master())\n","except ValueError:\n","    tpu = None\n","\n","if tpu:\n","    tf.config.experimental_connect_to_cluster(tpu)\n","    tf.tpu.experimental.initialize_tpu_system(tpu)\n","    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n","else:\n","    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n","\n","print(\"REPLICAS: \", strategy.num_replicas_in_sync)"]},{"cell_type":"markdown","metadata":{},"source":["# English to romanian MT\n"]},{"cell_type":"code","execution_count":6,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.execute_input":"2024-10-08T09:03:34.085896Z","iopub.status.busy":"2024-10-08T09:03:34.085572Z","iopub.status.idle":"2024-10-08T09:03:38.286207Z","shell.execute_reply":"2024-10-08T09:03:38.285225Z","shell.execute_reply.started":"2024-10-08T09:03:34.085843Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>English</th>\n","      <th>Vietnamese</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>One of the 3 owners of the 2024 Nobel Prize in...</td>\n","      <td>Một trong 3 chủ nhân của giải Nobel hóa học 20...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>From Hanoi, he sent a message to Vietnamese st...</td>\n","      <td>Từ Hà Nội, ông đã gửi lời nhắn tới sinh viên V...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Recently, on her personal page, Dr. Le Mai Lan...</td>\n","      <td>Mới đây, trên trang cá nhân của mình, TS Lê Ma...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Sharing with Thanh Nien Newspaper, Dr. Le Mai ...</td>\n","      <td>Chia sẻ với Báo Thanh Niên, TS Lê Mai Lan nhớ ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Before the award ceremony, I met and interview...</td>\n","      <td>Trước lễ trao giải, tôi đã gặp và phỏng vấn Jo...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>She is very young, with shaggy hair, khaki pan...</td>\n","      <td>Cô ấy rất trẻ, tóc tai bù xù, quần kaki, áo le...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>At that time, I saw that she was too impressed...</td>\n","      <td>Khi đó tôi đã thấy bạn ấy quá ấn tượng, công t...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>At that moment, I firmly believed that she wou...</td>\n","      <td>Cũng ngay lúc đó tôi đã tin tưởng chắc chắn rằ...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>The other scientist, the co-owner of the speci...</td>\n","      <td>Nhà khoa học còn lại, đồng chủ nhân giải đặc b...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Dr. Demis Hassabis is also one of three scient...</td>\n","      <td>TS Demis Hassabis cũng là một trong số ba nhà ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                             English  \\\n","0  One of the 3 owners of the 2024 Nobel Prize in...   \n","1  From Hanoi, he sent a message to Vietnamese st...   \n","2  Recently, on her personal page, Dr. Le Mai Lan...   \n","3  Sharing with Thanh Nien Newspaper, Dr. Le Mai ...   \n","4  Before the award ceremony, I met and interview...   \n","5  She is very young, with shaggy hair, khaki pan...   \n","6  At that time, I saw that she was too impressed...   \n","7  At that moment, I firmly believed that she wou...   \n","8  The other scientist, the co-owner of the speci...   \n","9  Dr. Demis Hassabis is also one of three scient...   \n","\n","                                          Vietnamese  \n","0  Một trong 3 chủ nhân của giải Nobel hóa học 20...  \n","1  Từ Hà Nội, ông đã gửi lời nhắn tới sinh viên V...  \n","2  Mới đây, trên trang cá nhân của mình, TS Lê Ma...  \n","3  Chia sẻ với Báo Thanh Niên, TS Lê Mai Lan nhớ ...  \n","4  Trước lễ trao giải, tôi đã gặp và phỏng vấn Jo...  \n","5  Cô ấy rất trẻ, tóc tai bù xù, quần kaki, áo le...  \n","6  Khi đó tôi đã thấy bạn ấy quá ấn tượng, công t...  \n","7  Cũng ngay lúc đó tôi đã tin tưởng chắc chắn rằ...  \n","8  Nhà khoa học còn lại, đồng chủ nhân giải đặc b...  \n","9  TS Demis Hassabis cũng là một trong số ba nhà ...  "]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["data = pd.read_csv(\"Data/Data_v2.csv\")\n","data = data.dropna()\n","data.head(10)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T09:03:38.287738Z","iopub.status.busy":"2024-10-08T09:03:38.287438Z","iopub.status.idle":"2024-10-08T09:03:38.298454Z","shell.execute_reply":"2024-10-08T09:03:38.297514Z","shell.execute_reply.started":"2024-10-08T09:03:38.287711Z"},"trusted":true},"outputs":[{"data":{"text/plain":["0    One of the 3 owners of the 2024 Nobel Prize in...\n","1    From Hanoi, he sent a message to Vietnamese st...\n","2    Recently, on her personal page, Dr. Le Mai Lan...\n","3    Sharing with Thanh Nien Newspaper, Dr. Le Mai ...\n","4    Before the award ceremony, I met and interview...\n","Name: English, dtype: object"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["eng_data = data['English']\n","eng_data.head()"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T09:03:53.805851Z","iopub.status.busy":"2024-10-08T09:03:53.805408Z","iopub.status.idle":"2024-10-08T09:03:53.813058Z","shell.execute_reply":"2024-10-08T09:03:53.812033Z","shell.execute_reply.started":"2024-10-08T09:03:53.805817Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(125, 2)"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["data.shape"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T09:03:53.815596Z","iopub.status.busy":"2024-10-08T09:03:53.815268Z","iopub.status.idle":"2024-10-08T09:03:54.107796Z","shell.execute_reply":"2024-10-08T09:03:54.106650Z","shell.execute_reply.started":"2024-10-08T09:03:53.815568Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>English</th>\n","      <th>Vietnamese</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ssss One of the 3 owners of the 2024 Nobel Pri...</td>\n","      <td>Một trong 3 chủ nhân của giải Nobel hóa học 20...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ssss From Hanoi, he sent a message to Vietname...</td>\n","      <td>Từ Hà Nội, ông đã gửi lời nhắn tới sinh viên V...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ssss Recently, on her personal page, Dr. Le Ma...</td>\n","      <td>Mới đây, trên trang cá nhân của mình, TS Lê Ma...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>ssss Sharing with Thanh Nien Newspaper, Dr. Le...</td>\n","      <td>Chia sẻ với Báo Thanh Niên, TS Lê Mai Lan nhớ ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ssss Before the award ceremony, I met and inte...</td>\n","      <td>Trước lễ trao giải, tôi đã gặp và phỏng vấn Jo...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                             English  \\\n","0  ssss One of the 3 owners of the 2024 Nobel Pri...   \n","1  ssss From Hanoi, he sent a message to Vietname...   \n","2  ssss Recently, on her personal page, Dr. Le Ma...   \n","3  ssss Sharing with Thanh Nien Newspaper, Dr. Le...   \n","4  ssss Before the award ceremony, I met and inte...   \n","\n","                                          Vietnamese  \n","0  Một trong 3 chủ nhân của giải Nobel hóa học 20...  \n","1  Từ Hà Nội, ông đã gửi lời nhắn tới sinh viên V...  \n","2  Mới đây, trên trang cá nhân của mình, TS Lê Ma...  \n","3  Chia sẻ với Báo Thanh Niên, TS Lê Mai Lan nhớ ...  \n","4  Trước lễ trao giải, tôi đã gặp và phỏng vấn Jo...  "]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# Append <START> and <END> to each english sentence\n","START = 'ssss '\n","END = ' eeee'\n","\n","data['English'] = data['English'].apply(lambda x: START+x+END)\n","data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T09:03:54.112036Z","iopub.status.busy":"2024-10-08T09:03:54.111575Z","iopub.status.idle":"2024-10-08T09:03:54.142431Z","shell.execute_reply":"2024-10-08T09:03:54.141172Z","shell.execute_reply.started":"2024-10-08T09:03:54.111989Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["ssss She is very young, with shaggy hair, khaki pants, and a turtleneck sweater, not much different from the students I still meet every day at VinUni. eeee\n","Cô ấy rất trẻ, tóc tai bù xù, quần kaki, áo len cổ lọ, không khác nhiều các sinh viên mà tôi vẫn gặp hàng ngày ở VinUni.\n","ssss At that time, I saw that she was too impressed, the work that her group did was so great. eeee\n","Khi đó tôi đã thấy bạn ấy quá ấn tượng, công trình mà nhóm bạn ấy làm quá vĩ đại.\n","ssss At that moment, I firmly believed that she would go a long way, and felt honored to meet such a wonderful person!\" eeee\n","Cũng ngay lúc đó tôi đã tin tưởng chắc chắn rằng bạn ấy sẽ còn tiến rất xa, và cảm thấy vinh hạnh được tiếp xúc một người tuyệt vời như vậy!\".\n"]}],"source":["eng_text = data['English'].tolist()\n","deu_text = data['Vietnamese'].tolist()\n","\n","print(eng_text[5])\n","print(deu_text[5])\n","print(eng_text[6])\n","print(deu_text[6])\n","print(eng_text[7])\n","print(deu_text[7])"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T09:03:54.144638Z","iopub.status.busy":"2024-10-08T09:03:54.144268Z","iopub.status.idle":"2024-10-08T09:03:54.185851Z","shell.execute_reply":"2024-10-08T09:03:54.184692Z","shell.execute_reply.started":"2024-10-08T09:03:54.144606Z"},"trusted":true},"outputs":[],"source":["num_words = 10000\n","\n","class TokenizerWrap(Tokenizer):\n","    \"\"\"\n","    Wrap the Tokenizer-class from Keras with more functionality.\n","    mở rộng từ Tokenizer của Keras với nhiều chức năng hơn.\n","    \"\"\"\n","    \n","    def __init__(self, texts, padding,\n","                 reverse=False, num_words=None):\n","        \"\"\"\n","        :param texts: List of strings. This is the data-set.\n","        :param padding: Either 'post' or 'pre' padding.\n","        :param reverse: Boolean whether to reverse token-lists.\n","        :param num_words: Max number of words to use.\n","        \"\"\"\n","\n","        Tokenizer.__init__(self, num_words=num_words)\n","\n","        # Create the vocabulary from the texts.\n","        # Tạo từ điển từ các từ trong văn bản, đều là các unique word thuộc cả 2 ngôn ngữ, \n","        # phương thức cũng ánh xạ luôn mỗi từ thành 1 số nguyên duy nhất.\n","        self.fit_on_texts(texts)\n","\n","        # Create inverse lookup from integer-tokens to words.\n","        # Tạo ánh xạ ngược từ số nguyên thành từ.\n","        self.index_to_word = dict(zip(self.word_index.values(),\n","                                      self.word_index.keys()))\n","\n","        # Convert all texts to lists of integer-tokens.\n","        # Note that the sequences may have different lengths.\n","        # Chuyển đổi tất cả các văn bản thành danh sách các token số nguyên (self.tokens).\n","        self.tokens = self.texts_to_sequences(texts)\n","\n","        if reverse:\n","            # Reverse the token-sequences.\n","            self.tokens = [list(reversed(x)) for x in self.tokens]\n","        \n","            # Sequences that are too long should now be truncated\n","            # at the beginning, which corresponds to the end of\n","            # the original sequences.\n","            truncating = 'pre'\n","        else:\n","            # Sequences that are too long should be truncated\n","            # at the end.\n","            truncating = 'post'\n","\n","        # The number of integer-tokens in each sequence.\n","        # Tính toán số lượng token trong mỗi sequence.\n","        self.num_tokens = [len(x) for x in self.tokens]\n","\n","        # Max number of tokens to use in all sequences.\n","        # We will pad / truncate all sequences to this length.\n","        # This is a compromise so we save a lot of memory and\n","        # only have to truncate maybe 5% of all the sequences.\n","        # Tính toán số lượng token tối đa để sử dụng trong tất cả các sequence.\n","        self.max_tokens = np.mean(self.num_tokens) \\\n","                          + 2 * np.std(self.num_tokens)\n","        self.max_tokens = int(self.max_tokens)\n","\n","        # Pad / truncate all token-sequences to the given length.\n","        # This creates a 2-dim numpy matrix that is easier to use.\n","        # Thực hiện padding hoặc cắt ngắn tất cả các danh sách token để có độ dài cố định.\n","        self.tokens_padded = pad_sequences(self.tokens,\n","                                           maxlen=self.max_tokens,\n","                                           padding=padding,\n","                                           truncating=truncating)\n","\n","    def token_to_word(self, token):\n","        \"\"\"\n","        Lookup a single word from an integer-token.\n","        Chuyển đổi một token số nguyên thành từ tương ứng\n","        \"\"\"\n","\n","        word = \" \" if token == 0 else self.index_to_word[token]\n","        return word \n","\n","    def tokens_to_string(self, tokens):\n","        \"\"\"\n","        Convert a list of integer-tokens to a string.\n","        Chuyển đổi một danh sách các token số nguyên thành một chuỗi văn bản.\n","        \"\"\"\n","\n","        # Create a list of the individual words.\n","        words = [self.index_to_word[token]\n","                 for token in tokens\n","                 if token != 0]\n","        \n","        # Concatenate the words to a single string\n","        # with space between all the words.\n","        text = \" \".join(words)\n","\n","        return text\n","    \n","    def text_to_tokens(self, text, reverse=False, padding=False):\n","        \"\"\"\n","        Convert a single text-string to tokens with optional\n","        reversal and padding.\n","        Chuyển đổi một chuỗi văn bản thành danh sách các token số nguyên, \n","        với tùy chọn đảo ngược và padding.\n","        \"\"\"\n","\n","        # Convert to tokens. Note that we assume there is only\n","        # a single text-string so we wrap it in a list.\n","        tokens = self.texts_to_sequences([text])\n","        tokens = np.array(tokens)\n","\n","        if reverse:\n","            # Reverse the tokens.\n","            tokens = np.flip(tokens, axis=1)\n","\n","            # Sequences that are too long should now be truncated\n","            # at the beginning, which corresponds to the end of\n","            # the original sequences.\n","            truncating = 'pre'\n","        else:\n","            # Sequences that are too long should be truncated\n","            # at the end.\n","            truncating = 'post'\n","\n","        if padding:\n","            # Pad and truncate sequences to the given length.\n","            tokens = pad_sequences(tokens,\n","                                   maxlen=self.max_tokens,\n","                                   padding='pre',\n","                                   truncating=truncating)\n","\n","        return tokens"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T09:03:54.190078Z","iopub.status.busy":"2024-10-08T09:03:54.189756Z","iopub.status.idle":"2024-10-08T09:04:32.650638Z","shell.execute_reply":"2024-10-08T09:04:32.649454Z","shell.execute_reply.started":"2024-10-08T09:03:54.190049Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: total: 15.6 ms\n","Wall time: 23.4 ms\n"]}],"source":["%%time\n","tokenizer_src = TokenizerWrap(texts=deu_text,\n","                              padding='pre',\n","                              reverse=True,\n","                              num_words=num_words\n","                             )"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T09:04:32.652380Z","iopub.status.busy":"2024-10-08T09:04:32.652048Z","iopub.status.idle":"2024-10-08T09:04:59.240273Z","shell.execute_reply":"2024-10-08T09:04:59.238986Z","shell.execute_reply.started":"2024-10-08T09:04:32.652349Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: total: 0 ns\n","Wall time: 13.7 ms\n"]}],"source":["%%time\n","tokenizer_des = TokenizerWrap(texts=eng_text,\n","                              padding='post',\n","                              reverse=False,\n","                              num_words=num_words\n","                             )"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T09:04:59.242310Z","iopub.status.busy":"2024-10-08T09:04:59.241834Z","iopub.status.idle":"2024-10-08T09:04:59.250841Z","shell.execute_reply":"2024-10-08T09:04:59.249673Z","shell.execute_reply.started":"2024-10-08T09:04:59.242262Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(125, 59)\n","(125, 48)\n"]}],"source":["token_src = tokenizer_src.tokens_padded\n","token_des = tokenizer_des.tokens_padded\n","\n","print(token_src.shape)\n","print(token_des.shape)"]},{"cell_type":"markdown","metadata":{},"source":["* ở đây có thể thấy có khoảng 400k câu, mỗi câu trong tiếng roma (src) chứa khoảng 50 token sau khi bị padding hoặc cắt ngắn\n","* tương tự với các câu tiếng anh (des) chứa khoảng 54 token sau khi xử lý. <br>\n","(2 thằng này có shape khác nhau do độ dài max của các ngôn ngữ khác nhau. )"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T09:04:59.252550Z","iopub.status.busy":"2024-10-08T09:04:59.252122Z","iopub.status.idle":"2024-10-08T09:04:59.265407Z","shell.execute_reply":"2024-10-08T09:04:59.264302Z","shell.execute_reply.started":"2024-10-08T09:04:59.252517Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2\n","3\n"]}],"source":["token_start = tokenizer_des.word_index[START.strip()]\n","token_end = tokenizer_des.word_index[END.strip()]\n","\n","print(token_start)\n","print(token_end)"]},{"cell_type":"markdown","metadata":{},"source":["## Model"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T09:04:59.266891Z","iopub.status.busy":"2024-10-08T09:04:59.266577Z","iopub.status.idle":"2024-10-08T09:04:59.276586Z","shell.execute_reply":"2024-10-08T09:04:59.275782Z","shell.execute_reply.started":"2024-10-08T09:04:59.266835Z"},"trusted":true},"outputs":[],"source":["encoder_inp_data = token_src\n","decoder_inp_data = token_des[:, :-1]\n","decoder_out_data = token_des[:, 1:]"]},{"cell_type":"markdown","metadata":{},"source":["### Encoder Model"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T09:04:59.278259Z","iopub.status.busy":"2024-10-08T09:04:59.277970Z","iopub.status.idle":"2024-10-08T09:04:59.288247Z","shell.execute_reply":"2024-10-08T09:04:59.287509Z","shell.execute_reply.started":"2024-10-08T09:04:59.278231Z"},"trusted":true},"outputs":[],"source":["# Glue all the encoder components together\n","def connect_encoder():\n","    net = encoder_input\n","    \n","    net = encoder_emb(net)\n","    net = encoder_gru1(net)\n","    net = encoder_gru2(net)\n","    out = encoder_gru3(net)\n","    \n","    return out"]},{"cell_type":"markdown","metadata":{},"source":["### Decoder Model"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T09:04:59.290330Z","iopub.status.busy":"2024-10-08T09:04:59.289646Z","iopub.status.idle":"2024-10-08T09:04:59.301454Z","shell.execute_reply":"2024-10-08T09:04:59.300686Z","shell.execute_reply.started":"2024-10-08T09:04:59.290293Z"},"trusted":true},"outputs":[],"source":["def connect_decoder(initial_state):    \n","    # Start the decoder-network with its input-layer.\n","    net = decoder_input\n","\n","    # Connect the embedding-layer.\n","    net = decoder_emb(net)\n","    \n","    # Connect all the GRU-layers.\n","    net = decoder_gru1(net, initial_state=initial_state)\n","    net = decoder_gru2(net, initial_state=initial_state)\n","    net = decoder_gru3(net, initial_state=initial_state)\n","\n","    # Connect the final dense layer that converts to\n","    # one-hot encoded arrays.\n","    decoder_output = decoder_dense(net)\n","    \n","    return decoder_output"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T09:04:59.302925Z","iopub.status.busy":"2024-10-08T09:04:59.302611Z","iopub.status.idle":"2024-10-08T09:05:06.611335Z","shell.execute_reply":"2024-10-08T09:05:06.610291Z","shell.execute_reply.started":"2024-10-08T09:04:59.302892Z"},"trusted":true},"outputs":[],"source":["# Connect all the models\n","with strategy.scope():\n","    \n","    embedding_size = 128\n","    state_size = 512\n","\n","    encoder_input = Input(shape=(None,), name='encoder_input')\n","    encoder_emb = Embedding(input_dim=num_words, output_dim=embedding_size, name='encoder_embedding')\n","\n","    encoder_gru1 = GRU(state_size, name='enc_gru1', return_sequences=True)\n","    encoder_gru2 = GRU(state_size, name='enc_gru2', return_sequences=True)\n","    encoder_gru3 = GRU(state_size, name='enc_gru3', return_sequences=False)\n","    \n","    encoder_op = connect_encoder()\n","    \n","    # Initial state placeholder takes a \"thought vector\" produced by the GRUs\n","    # That's why it needs the inputs with \"state_size\" (which was used in GRU size)\n","    decoder_initial_state = Input(shape=(state_size,), name='decoder_init_state')\n","\n","    # Decoder also needs an input, which is the basic input setence of the destination language\n","    decoder_input = Input(shape=(None,), name='decoder_input')\n","\n","    # Have the decoder embedding\n","    decoder_emb = Embedding(input_dim=num_words, output_dim=embedding_size, name='decoder_embedding')\n","\n","    # GRU arch similar to Encoder one with small changes\n","    decoder_gru1 = GRU(state_size, name='dec_gru1', return_sequences=True)\n","    decoder_gru2 = GRU(state_size, name='dec_gru2', return_sequences=True)\n","    decoder_gru3 = GRU(state_size, name='dec_gru3', return_sequences=True)\n","\n","    # Final dense layer for prediction\n","    decoder_dense = Dense(num_words, activation='softmax', name='decoder_output')\n","    decoder_op = connect_decoder(encoder_op)\n","    model_train = Model(inputs=[encoder_input, decoder_input],\n","                        outputs=[decoder_op])\n","    model_train.compile(optimizer=RMSprop(learning_rate=1e-3),\n","                        loss='sparse_categorical_crossentropy')"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T09:05:06.613153Z","iopub.status.busy":"2024-10-08T09:05:06.612748Z","iopub.status.idle":"2024-10-08T09:05:08.605882Z","shell.execute_reply":"2024-10-08T09:05:08.604559Z","shell.execute_reply.started":"2024-10-08T09:05:06.613114Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["You must install pydot (`pip install pydot`) for `plot_model` to work.\n"]}],"source":["tf.keras.utils.plot_model(model_train)"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["from tensorflow.keras.callbacks import Callback\n","from nltk.translate.bleu_score import sentence_bleu\n","\n","class BLEUScoreCallback(Callback):\n","    def __init__(self, tokenizer_src, tokenizer_des, val_data):\n","        self.tokenizer_src = tokenizer_src\n","        self.tokenizer_des = tokenizer_des\n","        self.val_data = val_data\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        val_inp, val_out = self.val_data\n","        predictions = self.model.predict(val_inp)\n","        bleu_scores = []\n","\n","        for i in range(len(predictions)):\n","            pred_sentence = self.tokenizer_des.tokens_to_string(predictions[i])\n","            ref_sentence = self.tokenizer_des.tokens_to_string(val_out[i])\n","            bleu_score = sentence_bleu([ref_sentence.split()], pred_sentence.split())\n","            bleu_scores.append(bleu_score)\n","\n","        avg_bleu_score = sum(bleu_scores) / len(bleu_scores)\n","        print(f' - val_bleu_score: {avg_bleu_score:.4f}')\n","# Sử dụng BLEUScoreCallback trong quá trình huấn luyện\n","val_data = (encoder_inp_data, decoder_out_data)\n","bleu_callback = BLEUScoreCallback(tokenizer_src, tokenizer_des, val_data)"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T09:05:08.611716Z","iopub.status.busy":"2024-10-08T09:05:08.611186Z","iopub.status.idle":"2024-10-08T09:05:08.617162Z","shell.execute_reply":"2024-10-08T09:05:08.616113Z","shell.execute_reply.started":"2024-10-08T09:05:08.611684Z"},"trusted":true},"outputs":[],"source":["path_checkpoint = '21_checkpoint.weights.h5'\n","callback_checkpoint = ModelCheckpoint(filepath=path_checkpoint,\n","                                      monitor='val_loss',\n","                                      verbose=1,\n","                                      save_weights_only=True,\n","                                      save_best_only=True)"]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T09:05:08.619070Z","iopub.status.busy":"2024-10-08T09:05:08.618659Z","iopub.status.idle":"2024-10-08T09:05:08.638266Z","shell.execute_reply":"2024-10-08T09:05:08.637488Z","shell.execute_reply.started":"2024-10-08T09:05:08.619023Z"},"trusted":true},"outputs":[],"source":["callback_early_stopping = EarlyStopping(monitor='val_loss',\n","                                        patience=3, verbose=1)"]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T09:05:08.639833Z","iopub.status.busy":"2024-10-08T09:05:08.639382Z","iopub.status.idle":"2024-10-08T09:05:08.650824Z","shell.execute_reply":"2024-10-08T09:05:08.649900Z","shell.execute_reply.started":"2024-10-08T09:05:08.639805Z"},"trusted":true},"outputs":[],"source":["callbacks = [callback_early_stopping,\n","             callback_checkpoint,\n","             #bleu_callback\n","             ]"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T09:05:08.652426Z","iopub.status.busy":"2024-10-08T09:05:08.652149Z","iopub.status.idle":"2024-10-08T09:05:08.664651Z","shell.execute_reply":"2024-10-08T09:05:08.663824Z","shell.execute_reply.started":"2024-10-08T09:05:08.652401Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Error trying to load checkpoint.\n","[Errno 2] Unable to synchronously open file (unable to open file: name = '21_checkpoint.weights.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n"]}],"source":["try:\n","    model_train.load_weights(path_checkpoint)\n","except Exception as error:\n","    print(\"Error trying to load checkpoint.\")\n","    print(error)"]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T09:05:08.666220Z","iopub.status.busy":"2024-10-08T09:05:08.665939Z","iopub.status.idle":"2024-10-08T09:05:08.678942Z","shell.execute_reply":"2024-10-08T09:05:08.678213Z","shell.execute_reply.started":"2024-10-08T09:05:08.666194Z"},"trusted":true},"outputs":[],"source":["x_data = {\n","    \"encoder_input\": encoder_inp_data,\n","    \"decoder_input\": decoder_inp_data\n","}\n","\n","y_data = {\n","    \"decoder_output\": decoder_out_data\n","}"]},{"cell_type":"code","execution_count":51,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T09:05:08.680297Z","iopub.status.busy":"2024-10-08T09:05:08.680012Z","iopub.status.idle":"2024-10-08T09:05:08.690727Z","shell.execute_reply":"2024-10-08T09:05:08.689913Z","shell.execute_reply.started":"2024-10-08T09:05:08.680272Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Validation Split: 0.8000%\n"]}],"source":["validation_split = 100/ len(encoder_inp_data)\n","print(f\"Validation Split: {validation_split:.4f}%\")"]},{"cell_type":"code","execution_count":52,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T09:05:08.692276Z","iopub.status.busy":"2024-10-08T09:05:08.692002Z","iopub.status.idle":"2024-10-08T09:13:51.934909Z","shell.execute_reply":"2024-10-08T09:13:51.933649Z","shell.execute_reply.started":"2024-10-08T09:05:08.692251Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n"]},{"ename":"ValueError","evalue":"Arguments `target` and `output` must have the same shape up until the last dimension: target.shape=(None, 47), output.shape=(None, 59, 10000)","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[52], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m strategy\u001b[38;5;241m.\u001b[39mscope():\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mmodel_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m384\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#validation_split=validation_split,\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\nn.py:659\u001b[0m, in \u001b[0;36msparse_categorical_crossentropy\u001b[1;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e1, e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape, output\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]):\n\u001b[0;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e1 \u001b[38;5;241m!=\u001b[39m e2:\n\u001b[1;32m--> 659\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    660\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArguments `target` and `output` must have the same shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    661\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mup until the last dimension: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    662\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, output.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    663\u001b[0m         )\n\u001b[0;32m    665\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m from_logits:\n\u001b[0;32m    666\u001b[0m     output \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mclip_by_value(\n\u001b[0;32m    667\u001b[0m         output, backend\u001b[38;5;241m.\u001b[39mepsilon(), \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m backend\u001b[38;5;241m.\u001b[39mepsilon()\n\u001b[0;32m    668\u001b[0m     )\n","\u001b[1;31mValueError\u001b[0m: Arguments `target` and `output` must have the same shape up until the last dimension: target.shape=(None, 47), output.shape=(None, 59, 10000)"]}],"source":["# Train the model\n","with strategy.scope():\n","    model_train.fit(\n","        x=x_data,\n","        y=y_data,\n","        batch_size=384,\n","        epochs=10,\n","        #validation_split=validation_split,\n","        callbacks=callbacks\n","    )"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T09:13:51.936938Z","iopub.status.busy":"2024-10-08T09:13:51.936544Z","iopub.status.idle":"2024-10-08T09:13:52.177177Z","shell.execute_reply":"2024-10-08T09:13:52.176047Z","shell.execute_reply.started":"2024-10-08T09:13:51.936906Z"},"trusted":true},"outputs":[],"source":["model_train.save(\"eng_to_deu.hdf5\")"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T09:27:29.627601Z","iopub.status.busy":"2024-10-08T09:27:29.627164Z","iopub.status.idle":"2024-10-08T09:27:29.633628Z","shell.execute_reply":"2024-10-08T09:27:29.632475Z","shell.execute_reply.started":"2024-10-08T09:27:29.627565Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import numpy as np\n","import pickle"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T10:01:02.011878Z","iopub.status.busy":"2024-10-08T10:01:02.010933Z","iopub.status.idle":"2024-10-08T10:01:02.016886Z","shell.execute_reply":"2024-10-08T10:01:02.015680Z","shell.execute_reply.started":"2024-10-08T10:01:02.011805Z"},"trusted":true},"outputs":[],"source":["\n","# Load the saved model\n","model = model_train"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T10:01:04.432023Z","iopub.status.busy":"2024-10-08T10:01:04.431609Z","iopub.status.idle":"2024-10-08T10:01:04.438884Z","shell.execute_reply":"2024-10-08T10:01:04.437978Z","shell.execute_reply.started":"2024-10-08T10:01:04.431990Z"},"trusted":true},"outputs":[{"data":{"text/plain":["<keras.engine.functional.Functional at 0x7c4cd80ad1d0>"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["model"]},{"cell_type":"markdown","metadata":{},"source":["# Test english to romanian/"]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T10:04:56.834674Z","iopub.status.busy":"2024-10-08T10:04:56.833654Z","iopub.status.idle":"2024-10-08T10:04:56.844231Z","shell.execute_reply":"2024-10-08T10:04:56.843316Z","shell.execute_reply.started":"2024-10-08T10:04:56.834625Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import numpy as np\n","\n","# Step 2: Preprocess the new sentence\n","def preprocess_sentence(sentence, tokenizer, max_length):\n","    sequence = tokenizer.texts_to_sequences([sentence])\n","    padded_sequence = pad_sequences(sequence, maxlen=max_length, padding='post')\n","    return padded_sequence\n","\n","# Step 3: Translate the sentence\n","def translate_sentence(model, sentence, tokenizer, max_length):\n","    preprocessed_sentence = preprocess_sentence(sentence, tokenizer, max_length)\n","    # Create a dummy decoder input (e.g., start token)\n","    decoder_input_data = np.zeros((1, max_length))\n","    decoder_input_data[0, 0] = tokenizer.word_index['start']  # Replace 'start' with your actual start token\n","    prediction = model.predict([preprocessed_sentence, decoder_input_data])\n","    return prediction\n","\n","# Step 4: Post-process the output\n","def decode_sequence(prediction, tokenizer):\n","    index_to_word = {index: word for word, index in tokenizer.word_index.items()}\n","    decoded_sentence = ' '.join([index_to_word.get(index, '') for index in np.argmax(prediction, axis=-1)[0]])\n","    return decoded_sentence\n"]},{"cell_type":"code","execution_count":54,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T10:05:24.939207Z","iopub.status.busy":"2024-10-08T10:05:24.938473Z","iopub.status.idle":"2024-10-08T10:05:25.003245Z","shell.execute_reply":"2024-10-08T10:05:25.002182Z","shell.execute_reply.started":"2024-10-08T10:05:24.939172Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Translated Sentence: applause                   \n"]}],"source":["\n","# Example usage\n","english_sentence = \"how are you\"\n","\n","# Assuming tokenizer_des and max_length are already defined\n","# Translate the sentence\n","translated_sentence = translate_sentence(model, english_sentence, tokenizer_des, max_length)\n","# Decode the translated sentence\n","romanian_sentence = decode_sequence(translated_sentence, tokenizer_des)\n","\n","print(\"Translated Sentence:\", romanian_sentence)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":288723,"sourceId":593577,"sourceType":"datasetVersion"}],"dockerImageVersionId":30299,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":4}
