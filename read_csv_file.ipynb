{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STT</th>\n",
       "      <th>English</th>\n",
       "      <th>Vietnamese</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>One of the 3 owners of the 2024 Nobel Prize in...</td>\n",
       "      <td>Một trong 3 chủ nhân của giải Nobel hóa học 20...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>From Hanoi, he sent a message to Vietnamese st...</td>\n",
       "      <td>Từ Hà Nội, ông đã gửi lời nhắn tới sinh viên V...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Recently, on her personal page, Dr. Le Mai Lan...</td>\n",
       "      <td>Mới đây, trên trang cá nhân của mình, TS Lê Ma...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Sharing with Thanh Nien Newspaper, Dr. Le Mai ...</td>\n",
       "      <td>Chia sẻ với Báo Thanh Niên, TS Lê Mai Lan nhớ ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Before the award ceremony, I met and interview...</td>\n",
       "      <td>Trước lễ trao giải, tôi đã gặp và phỏng vấn Jo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   STT                                            English  \\\n",
       "0    1  One of the 3 owners of the 2024 Nobel Prize in...   \n",
       "1    2  From Hanoi, he sent a message to Vietnamese st...   \n",
       "2    3  Recently, on her personal page, Dr. Le Mai Lan...   \n",
       "3    4  Sharing with Thanh Nien Newspaper, Dr. Le Mai ...   \n",
       "4    5  Before the award ceremony, I met and interview...   \n",
       "\n",
       "                                          Vietnamese  Unnamed: 3  Unnamed: 4  \n",
       "0  Một trong 3 chủ nhân của giải Nobel hóa học 20...         NaN         NaN  \n",
       "1  Từ Hà Nội, ông đã gửi lời nhắn tới sinh viên V...         NaN         NaN  \n",
       "2  Mới đây, trên trang cá nhân của mình, TS Lê Ma...         NaN         NaN  \n",
       "3  Chia sẻ với Báo Thanh Niên, TS Lê Mai Lan nhớ ...         NaN         NaN  \n",
       "4  Trước lễ trao giải, tôi đã gặp và phỏng vấn Jo...         NaN         NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./Data/Data_eng_vn.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter necessery data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>Vietnamese</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the 3 owners of the 2024 Nobel Prize in...</td>\n",
       "      <td>Một trong 3 chủ nhân của giải Nobel hóa học 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From Hanoi, he sent a message to Vietnamese st...</td>\n",
       "      <td>Từ Hà Nội, ông đã gửi lời nhắn tới sinh viên V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recently, on her personal page, Dr. Le Mai Lan...</td>\n",
       "      <td>Mới đây, trên trang cá nhân của mình, TS Lê Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sharing with Thanh Nien Newspaper, Dr. Le Mai ...</td>\n",
       "      <td>Chia sẻ với Báo Thanh Niên, TS Lê Mai Lan nhớ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Before the award ceremony, I met and interview...</td>\n",
       "      <td>Trước lễ trao giải, tôi đã gặp và phỏng vấn Jo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             English  \\\n",
       "0  One of the 3 owners of the 2024 Nobel Prize in...   \n",
       "1  From Hanoi, he sent a message to Vietnamese st...   \n",
       "2  Recently, on her personal page, Dr. Le Mai Lan...   \n",
       "3  Sharing with Thanh Nien Newspaper, Dr. Le Mai ...   \n",
       "4  Before the award ceremony, I met and interview...   \n",
       "\n",
       "                                          Vietnamese  \n",
       "0  Một trong 3 chủ nhân của giải Nobel hóa học 20...  \n",
       "1  Từ Hà Nội, ông đã gửi lời nhắn tới sinh viên V...  \n",
       "2  Mới đây, trên trang cá nhân của mình, TS Lê Ma...  \n",
       "3  Chia sẻ với Báo Thanh Niên, TS Lê Mai Lan nhớ ...  \n",
       "4  Trước lễ trao giải, tôi đã gặp và phỏng vấn Jo...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['English', 'Vietnamese']\n",
    "df = df[columns]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**create eng_data, vi_data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    One of the 3 owners of the 2024 Nobel Prize in...\n",
       "1    From Hanoi, he sent a message to Vietnamese st...\n",
       "2    Recently, on her personal page, Dr. Le Mai Lan...\n",
       "3    Sharing with Thanh Nien Newspaper, Dr. Le Mai ...\n",
       "4    Before the award ceremony, I met and interview...\n",
       "Name: English, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_data = df['English']\n",
    "eng_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>Vietnamese</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ssss One of the 3 owners of the 2024 Nobel Pri...</td>\n",
       "      <td>Một trong 3 chủ nhân của giải Nobel hóa học 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ssss From Hanoi, he sent a message to Vietname...</td>\n",
       "      <td>Từ Hà Nội, ông đã gửi lời nhắn tới sinh viên V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ssss Recently, on her personal page, Dr. Le Ma...</td>\n",
       "      <td>Mới đây, trên trang cá nhân của mình, TS Lê Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ssss Sharing with Thanh Nien Newspaper, Dr. Le...</td>\n",
       "      <td>Chia sẻ với Báo Thanh Niên, TS Lê Mai Lan nhớ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ssss Before the award ceremony, I met and inte...</td>\n",
       "      <td>Trước lễ trao giải, tôi đã gặp và phỏng vấn Jo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             English  \\\n",
       "0  ssss One of the 3 owners of the 2024 Nobel Pri...   \n",
       "1  ssss From Hanoi, he sent a message to Vietname...   \n",
       "2  ssss Recently, on her personal page, Dr. Le Ma...   \n",
       "3  ssss Sharing with Thanh Nien Newspaper, Dr. Le...   \n",
       "4  ssss Before the award ceremony, I met and inte...   \n",
       "\n",
       "                                          Vietnamese  \n",
       "0  Một trong 3 chủ nhân của giải Nobel hóa học 20...  \n",
       "1  Từ Hà Nội, ông đã gửi lời nhắn tới sinh viên V...  \n",
       "2  Mới đây, trên trang cá nhân của mình, TS Lê Ma...  \n",
       "3  Chia sẻ với Báo Thanh Niên, TS Lê Mai Lan nhớ ...  \n",
       "4  Trước lễ trao giải, tôi đã gặp và phỏng vấn Jo...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Append <START> and <END> to each english sentence\n",
    "START = 'ssss '\n",
    "END = ' eeee'\n",
    "\n",
    "df['English'] = df['English'].apply(lambda x: START+x+END)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssss She is very young, with shaggy hair, khaki pants, and a turtleneck sweater, not much different from the students I still meet every day at VinUni. eeee\n",
      "Bạn ấy rất trẻ, tóc tai bù xù, quần kaki, áo len cổ lọ, không khác nhiều các sinh viên mà tôi vẫn gặp hàng ngày ở VinUni.\n",
      "ssss At that time, I saw that she was too impressed, the work that her group did was so great. eeee\n",
      "Khi đó tôi đã thấy bạn ấy quá ấn tượng, công trình mà nhóm bạn ấy làm quá vĩ đại.\n",
      "ssss At that moment, I firmly believed that she would go a long way, and felt honored to meet such a wonderful person!\" eeee\n",
      "Cũng ngay lúc đó tôi đã tin tưởng chắc chắn rằng bạn ấy sẽ còn tiến rất xa, và cảm thấy vinh hạnh được tiếp xúc một người tuyệt vời như vậy!\".\n"
     ]
    }
   ],
   "source": [
    "eng_text = df['English'].tolist()\n",
    "vi_text = df['Vietnamese'].tolist()\n",
    "\n",
    "print(eng_text[5])\n",
    "print(vi_text[5])\n",
    "print(eng_text[6])\n",
    "print(vi_text[6])\n",
    "print(eng_text[7])\n",
    "print(vi_text[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"English Texts:\"\n",
      "['ssss One of the 3 owners of the 2024 Nobel Prize in Chemistry came to '\n",
      " 'Vietnam in 2022 to receive the VinFuture Prize. eeee',\n",
      " 'ssss From Hanoi, he sent a message to Vietnamese students to dare to dream '\n",
      " 'big things like himself in his youth. eeee',\n",
      " 'ssss Recently, on her personal page, Dr. Le Mai Lan, Chairman of the Board '\n",
      " 'of Trustees of VinUni University shared a photo she took with Dr. John '\n",
      " 'Jumper (born in 1985) from December 2022. eeee',\n",
      " 'ssss Sharing with Thanh Nien Newspaper, Dr. Le Mai Lan recalled: \"Dr. John '\n",
      " \"Jumper is one of the two scientists who won the special prize in VinFuture's \"\n",
      " 'new fields research 2022 thanks to the work of the artificial intelligence '\n",
      " 'system that decodes the AlphaFold 2 protein. eeee',\n",
      " 'ssss Before the award ceremony, I met and interviewed John at the Metropole '\n",
      " 'Hotel, Hanoi. eeee']\n",
      "Vietnamese Texts:\n",
      "['Một trong 3 chủ nhân của giải Nobel hóa học 2024 đã từng đến Việt Nam năm '\n",
      " '2022 để nhận giải thưởng VinFuture. ',\n",
      " 'Từ Hà Nội, ông đã gửi lời nhắn tới sinh viên Việt Nam hãy dám mơ những điều '\n",
      " 'lớn lao giống như chính ông thời trẻ.',\n",
      " 'Mới đây, trên trang cá nhân của mình, TS Lê Mai Lan, Chủ tịch Hội đồng '\n",
      " 'trường ĐH VinUni đã chia sẻ bức ảnh bà chụp với TS John Jumper (sinh năm '\n",
      " '1985) từ tháng 12.2022.',\n",
      " 'Chia sẻ với Báo Thanh Niên, TS Lê Mai Lan nhớ lại: \"TS John Jumper là một '\n",
      " 'trong hai nhà khoa học đạt giải đặc biệt trong nghiên cứu các lĩnh vực mới '\n",
      " 'của VinFuture 2022 nhờ công trình hệ thống trí tuệ nhân tạo giải mã protein '\n",
      " 'AlphaFold 2.',\n",
      " 'Trước lễ trao giải, tôi đã gặp và phỏng vấn John tại khách sạn Metropole, Hà '\n",
      " 'nội.']\n"
     ]
    }
   ],
   "source": [
    "print('\"English Texts:\"')\n",
    "pprint(eng_text[:5])\n",
    "print(\"Vietnamese Texts:\")\n",
    "pprint( vi_text[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Tokenizerwrap class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, GRU, Embedding\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TokenizerWrap(Tokenizer):\n",
    "    \"\"\"\n",
    "    Wrap the Tokenizer-class from Keras with more functionality.\n",
    "    mở rộng từ Tokenizer của Keras với nhiều chức năng hơn.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, texts, padding,\n",
    "                 reverse=False, num_words=None):\n",
    "        \"\"\"\n",
    "        :param texts: List of strings. This is the data-set.\n",
    "        :param padding: Either 'post' or 'pre' padding.\n",
    "        :param reverse: Boolean whether to reverse token-lists.\n",
    "        :param num_words: Max number of words to use.\n",
    "        \"\"\"\n",
    "\n",
    "        Tokenizer.__init__(self, num_words=num_words)\n",
    "\n",
    "        # Create the vocabulary from the texts.\n",
    "        # Tạo từ điển từ các từ trong văn bản, đều là các unique word thuộc cả 2 ngôn ngữ, \n",
    "        # phương thức cũng ánh xạ luôn mỗi từ thành 1 số nguyên duy nhất.\n",
    "        self.fit_on_texts(texts)\n",
    "\n",
    "        # Create inverse lookup from integer-tokens to words.\n",
    "        # Tạo ánh xạ ngược từ số nguyên thành từ.\n",
    "        self.index_to_word = dict(zip(self.word_index.values(),\n",
    "                                      self.word_index.keys()))\n",
    "\n",
    "        # Convert all texts to lists of integer-tokens.\n",
    "        # Note that the sequences may have different lengths.\n",
    "        # Chuyển đổi tất cả các văn bản thành danh sách các token số nguyên (self.tokens).\n",
    "        self.tokens = self.texts_to_sequences(texts)\n",
    "\n",
    "        if reverse:\n",
    "            # Reverse the token-sequences.\n",
    "            self.tokens = [list(reversed(x)) for x in self.tokens]\n",
    "        \n",
    "            # Sequences that are too long should now be truncated\n",
    "            # at the beginning, which corresponds to the end of\n",
    "            # the original sequences.\n",
    "            truncating = 'pre'\n",
    "        else:\n",
    "            # Sequences that are too long should be truncated\n",
    "            # at the end.\n",
    "            truncating = 'post'\n",
    "\n",
    "        # The number of integer-tokens in each sequence.\n",
    "        # Tính toán số lượng token trong mỗi sequence.\n",
    "        self.num_tokens = [len(x) for x in self.tokens]\n",
    "\n",
    "        # Max number of tokens to use in all sequences.\n",
    "        # We will pad / truncate all sequences to this length.\n",
    "        # This is a compromise so we save a lot of memory and\n",
    "        # only have to truncate maybe 5% of all the sequences.\n",
    "        # Tính toán số lượng token tối đa để sử dụng trong tất cả các sequence.\n",
    "        self.max_tokens = np.mean(self.num_tokens) \\\n",
    "                          + 2 * np.std(self.num_tokens)\n",
    "        self.max_tokens = int(self.max_tokens)\n",
    "\n",
    "        # Pad / truncate all token-sequences to the given length.\n",
    "        # This creates a 2-dim numpy matrix that is easier to use.\n",
    "        # Thực hiện padding hoặc cắt ngắn tất cả các danh sách token để có độ dài cố định.\n",
    "        self.tokens_padded = pad_sequences(self.tokens,\n",
    "                                           maxlen=self.max_tokens,\n",
    "                                           padding=padding,\n",
    "                                           truncating=truncating)\n",
    "\n",
    "    def token_to_word(self, token):\n",
    "        \"\"\"\n",
    "        Lookup a single word from an integer-token.\n",
    "        Chuyển đổi một token số nguyên thành từ tương ứng\n",
    "        \"\"\"\n",
    "\n",
    "        word = \" \" if token == 0 else self.index_to_word[token]\n",
    "        return word \n",
    "\n",
    "    def tokens_to_string(self, tokens):\n",
    "        \"\"\"\n",
    "        Convert a list of integer-tokens to a string.\n",
    "        Chuyển đổi một danh sách các token số nguyên thành một chuỗi văn bản.\n",
    "        \"\"\"\n",
    "\n",
    "        # Create a list of the individual words.\n",
    "        words = [self.index_to_word[token]\n",
    "                 for token in tokens\n",
    "                 if token != 0]\n",
    "        \n",
    "        # Concatenate the words to a single string\n",
    "        # with space between all the words.\n",
    "        text = \" \".join(words)\n",
    "\n",
    "        return text\n",
    "    \n",
    "    def text_to_tokens(self, text, reverse=False, padding=False):\n",
    "        \"\"\"\n",
    "        Convert a single text-string to tokens with optional\n",
    "        reversal and padding.\n",
    "        Chuyển đổi một chuỗi văn bản thành danh sách các token số nguyên, \n",
    "        với tùy chọn đảo ngược và padding.\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert to tokens. Note that we assume there is only\n",
    "        # a single text-string so we wrap it in a list.\n",
    "        tokens = self.texts_to_sequences([text])\n",
    "        tokens = np.array(tokens)\n",
    "\n",
    "        if reverse:\n",
    "            # Reverse the tokens.\n",
    "            tokens = np.flip(tokens, axis=1)\n",
    "\n",
    "            # Sequences that are too long should now be truncated\n",
    "            # at the beginning, which corresponds to the end of\n",
    "            # the original sequences.\n",
    "            truncating = 'pre'\n",
    "        else:\n",
    "            # Sequences that are too long should be truncated\n",
    "            # at the end.\n",
    "            truncating = 'post'\n",
    "\n",
    "        if padding:\n",
    "            # Pad and truncate sequences to the given length.\n",
    "            tokens = pad_sequences(tokens,\n",
    "                                   maxlen=self.max_tokens,\n",
    "                                   padding='pre',\n",
    "                                   truncating=truncating)\n",
    "\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 10000\n",
    "\n",
    "# Khởi tạo đối tượng TokenizerWrap cho tiếng Anh\n",
    "tokenizer_eng = TokenizerWrap(texts=eng_text, padding='post', reverse=False, num_words=num_words)\n",
    "\n",
    "# Khởi tạo đối tượng TokenizerWrap cho tiếng Việt\n",
    "tokenizer_vi = TokenizerWrap(texts=vi_text, padding='pre', reverse=True, num_words=num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word to index and index to word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Word Index:\n",
      "[('the', 1),\n",
      " ('ssss', 2),\n",
      " ('eeee', 3),\n",
      " ('of', 4),\n",
      " ('to', 5),\n",
      " ('and', 6),\n",
      " ('a', 7),\n",
      " ('in', 8),\n",
      " ('is', 9),\n",
      " ('that', 10)]\n",
      "Vietnamese Word Index:\n",
      "[('có', 1),\n",
      " ('là', 2),\n",
      " ('người', 3),\n",
      " ('và', 4),\n",
      " ('một', 5),\n",
      " ('của', 6),\n",
      " ('được', 7),\n",
      " ('cho', 8),\n",
      " ('với', 9),\n",
      " ('không', 10)]\n",
      "English Index to Word:\n",
      "[(1, 'the'),\n",
      " (2, 'ssss'),\n",
      " (3, 'eeee'),\n",
      " (4, 'of'),\n",
      " (5, 'to'),\n",
      " (6, 'and'),\n",
      " (7, 'a'),\n",
      " (8, 'in'),\n",
      " (9, 'is'),\n",
      " (10, 'that')]\n",
      "Vietnamese Index to Word:\n",
      "[(1, 'có'),\n",
      " (2, 'là'),\n",
      " (3, 'người'),\n",
      " (4, 'và'),\n",
      " (5, 'một'),\n",
      " (6, 'của'),\n",
      " (7, 'được'),\n",
      " (8, 'cho'),\n",
      " (9, 'với'),\n",
      " (10, 'không')]\n"
     ]
    }
   ],
   "source": [
    "# Hiển thị từ điển và ánh xạ ngược\n",
    "print(\"English Word Index:\" )\n",
    "pprint(list(tokenizer_eng.word_index.items())[:10])\n",
    "\n",
    "print(\"Vietnamese Word Index:\")\n",
    "pprint(list(tokenizer_vi.word_index.items())[:10])\n",
    "\n",
    "print(\"English Index to Word:\")\n",
    "pprint(list(tokenizer_eng.index_to_word.items())[:10])\n",
    "\n",
    "print(\"Vietnamese Index to Word:\")\n",
    "pprint(list(tokenizer_vi.index_to_word.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of token each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokens in English Sentences:\n",
      "[24, 22, 35, 47, 16]\n",
      "Number of Tokens in Vietnamese Sentences:\n",
      "[23, 26, 37, 50, 17]\n"
     ]
    }
   ],
   "source": [
    "# Hiển thị số lượng token trong mỗi câu\n",
    "print(\"Number of Tokens in English Sentences:\")\n",
    "pprint(tokenizer_eng.num_tokens[:5])\n",
    "\n",
    "print(\"Number of Tokens in Vietnamese Sentences:\")\n",
    "pprint(tokenizer_vi.num_tokens[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Số token tối đa ở mỗi dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Tokens in English:\n",
      "48\n",
      "Max Tokens in Vietnamese:\n",
      "59\n"
     ]
    }
   ],
   "source": [
    "# Hiển thị số lượng token tối đa\n",
    "print(\"Max Tokens in English:\")\n",
    "pprint(tokenizer_eng.max_tokens)\n",
    "\n",
    "print(\"Max Tokens in Vietnamese:\")\n",
    "pprint(tokenizer_vi.max_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nếu chọn số token tối đa làm số chiều của mỗi vector nhúng thì cần padding cho những câu không đủ max token. Ví dụ câu số 1 bản eng có 10 token (ví dụ), thì mình add thêm 38 token mang giá trị 0 (ae thấy ở trên index bắt đầu từ 1, còn 0 để cho phần padding này) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ouput của bước này là vector nào trong eng cx đủ 48, vi đủ 50 nhá"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "à mục đích của bước tiền xử lý là biểu diễn các câu(sample) dưới dạng vector số cùng chiều nhé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded English Tokens:\n",
      "array([[  2,  42,   4,   1, 241, 432,   4,   1,  43,  44,  22,   8, 242,\n",
      "        243,   5,  50,   8,  84,   5, 111,   1,  85,  22,   3,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "       [  2,  14, 244, 245, 433,   7, 434,   5,  64,  51,   5, 435,   5,\n",
      "        246, 154, 247, 112, 436,   8, 437, 438,   3,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "       [  2, 248,  17,  23, 155, 439,  24, 113, 156, 157, 440,   4,   1,\n",
      "        441,   4, 442,   4, 249, 114,  86,   7, 443,  37, 444,  12,  24,\n",
      "         87, 115, 445,   8, 446,  14, 447,  84,   3,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "       [  2, 250,  12, 448, 449, 450,  24, 113, 156, 157, 451,  24,  87,\n",
      "        115,   9,  42,   4,   1, 251, 116,  29, 158,   1, 252,  22,   8,\n",
      "        452,  34, 253,  52,  84, 453,   5,   1, 117,   4,   1, 254, 159,\n",
      "        454,  10, 455,   1, 255, 118, 456,   3,   0],\n",
      "       [  2, 256,   1,  65, 457,  18, 458,   6, 459,  87,  15,   1, 460,\n",
      "        461, 244,   3,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0]])\n",
      "Padded Vietnamese Tokens:\n",
      "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 147, 123,  15,\n",
      "         45,  43, 185,  16,  42,  39,  23, 305,  13, 101,  12, 184, 100,\n",
      "         15,   6,  38, 236, 235,  11,   5],\n",
      "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0, 124, 237, 307, 148,  69, 599,\n",
      "        308, 103,  24,  20, 395, 598, 394,  42,  39,  56,  30, 102, 597,\n",
      "        596, 393,  13, 307, 306, 392,  18],\n",
      "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0, 185, 604, 187,  18,\n",
      "        603,  16,  30, 239, 186,  57,   9, 602,  21, 400, 601, 126, 125,\n",
      "         13, 399, 311,  70,  46, 149, 398, 236, 310, 309, 238,  57, 397,\n",
      "          6,  38, 396, 600, 105,  83, 104],\n",
      "       [  0,   0,   0,   0,   0,   0,   0,   0,   0, 191, 402, 608, 607,\n",
      "         15, 128,  38, 401, 190, 242, 312, 189,  25, 606, 185, 147,   6,\n",
      "        104,  84, 127,  17,  74,  73,  11, 153, 152,  15, 151,  12,  72,\n",
      "         34, 241,  11,   5,   2, 239, 186,  57,  71, 605, 310, 309, 238,\n",
      "         57, 150, 240, 188,   9, 126, 125],\n",
      "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0, 306, 392, 612, 611, 610, 154, 186, 192, 609,   4,\n",
      "        313,  13,  14,  15, 243, 403, 129]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hiển thị các câu đã được padding\n",
    "print(\"Padded English Tokens:\")\n",
    "pprint(tokenizer_eng.tokens_padded[:5])\n",
    "\n",
    "print(\"Padded Vietnamese Tokens:\")\n",
    "pprint(tokenizer_vi.tokens_padded[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
